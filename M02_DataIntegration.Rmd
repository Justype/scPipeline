---
title: "Data Integration"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  flexdashboard::flex_dashboard:
    orientation: rows
    vertical_layout: fill
    source_code: embed
    theme: flatly
    navbar:
      - { title: "scPipeline", href: "https://github.com/NMikolajewicz/scPipeline" }
      - { title: "scMiko", href: "https://github.com/NMikolajewicz/scMiko" }  
editor_options: 
  chunk_output_type: inline
knit: (function(inputFile, encoding) {
    rmarkdown::render(input = inputFile,
      encoding = encoding,
      output_file = if (exists("user")){paste0(
        xfun::sans_ext(inputFile), '_',user, "_", 
        paste0(format(Sys.time(), "%d_%b_"), gsub(" ", "_", gsub(":", "_", format(Sys.time(), "%X"))), format(Sys.time(), "_%Y")), '.html'
      )} else {paste0(xfun::sans_ext(inputFile), '_',"Guest", "_", 
      paste0(format(Sys.time(), "%d_%b_"), gsub(" ", "_", gsub(":", "_", format(Sys.time(), "%X"))), format(Sys.time(), "_%Y")), '.html')},
      output_dir = if (exists("data.path")) paste0(data.path, "/HTML_Reports") else NULL
    )
  })
---



```{r setup, include=FALSE}

# clear global enviroment
rm(list = setdiff(ls(), c("data.path", "user")))
invisible({gc()})

# initiate timer
start.time <- proc.time()

# List of packages to load
packages2load <- c("Seurat", "sctransform",
                    "dplyr", "tidyr", "RColorBrewer", "ggplot2", "gridExtra", 
                   "DT", "flexdashboard", "future", "biomaRt", "foreach", "parallel", "doParallel", "scMiko", "reshape2", "glmGamPoi")
# load packages
invisible({lapply(packages2load, library, character.only = TRUE)})

```


```{r parameter specification }

parameter.list <- list(
  save.filename = "p9_GBM_PR_integrated_SCTrerun_110521",
  integration.k.filter =  200, # lower integration.k.filter value (e.g., 50) if there is few cells. Default = 200
  integration.n.genes = 3000,  # number of genes for integration: 1000-3000 recommended
  integration.method = "rPCA", # options: CCA, rPCA
  integration.limit.memory = T, # specify whether to limit max memory (set T if local)
  integration.max.memory = 150, # numeric, in terms of Gb (set to 100 for cluster)
  integration.cluster.resolution = 0.5, 
  integration.rerun.SCT = F,
  n.workers = list(
    import = 4,
    integration = 2 #(limit to 1 for cluster)
  ),
  print.inline = FALSE,
  save.integrated.object = T,
  save.pdf = T,
  update.log = T
)

subset.all <- 1
input.data <- list(
  R1 = list(
    file = "Module1_p9_GBM_PR_noFilter_270620.Rdata",
    integrate.by = "Barcode",
    subsample = subset.all,
    grouping.field = NA,
    groups = NA))



# input.data <- list(
#   p13.renca = list(
#     file = "R415_M01_NM2_pilot12_meso_v2_160421.Rdata",
#     cluster.resolution = 0.5,
#     integrate.by = "Barcode",
#     species = "Hs",
#     subsample = subset.all,
#     grouping.field = NA,
#     groups = NA))


# save.integrated.object <- T
# 
# save.filename <- "p12_meso_integrated_170421"
# 
# integration.k.filter <-  200 # lower integration.k.filter value (e.g., 50) if there is few cells. Default = 200
# 
# integration.n.genes <- 2000  # number of features used for data integration # if datasets are divergent, may need fewer genes to align with (e.g., 1000)
# 
# integration.method <- "rPCA" # "CCA" for small datasets, "rPCA" for large datasets
# 
# print.inline <- FALSE

# n.workers <- list(
#   import = 4,
#   integration = 1 #(limit to 1 for cluster)
# )

  # integration.limit.memory <- T # specify whether to limit max memory (set T if local)
  # integration.max.memory <- 150 # numeric, in terms of Gb (set to 100 for cluster)



# target.species <- "Hs" #if mixed input, specify target as 'Hs' for best performance





# input data specification
# input.data list headers: file, cluster.resolution, integrate.by, species, subsample, grouping.field, groups

# input.data <- list(
#   p13.renca = list(
#     file = "R397_M01_NM2_pilot13_celsius_renca_230221.Rdata",
#     cluster.resolution = 0.3,
#     integrate.by = "Barcode",
#     species = "Mm",
#     subsample = subset.all,
#     grouping.field = NA,
#     groups = NA)
# )
# R397_M01_NM2_pilot13_celsius_renca_230221.Rdata # filter
# R398_M01_NM2_pilot13_celsius_renca_230221.Rdata # no filter



```


```{r parameter assignment and assertion statements}


dir <- "Preprocessed_Datasets/"

if ("integration.k.filter" %in% names(parameter.list)){
  stopifnot(is.numeric(parameter.list$integration.k.filter))
} else {parameter.list$integration.k.filter <- 200}

# ensure output file is specified 
if (parameter.list$save.integrated.object){
  stopifnot("save.filename" %in% names(parameter.list))
  if (!(grepl(".Rdata", parameter.list$save.filename))){
    parameter.list$save.filename <- paste0(parameter.list$save.filename, ".Rdata")
  }
}

n.cor.available <- parallel::detectCores()
for (i in 1:length(parameter.list$n.workers)){
  if (parameter.list$n.workers[[i]] > n.cor.available) parameter.list$n.workers[[i]] <- n.cor.available
}

input.labels <- names(input.data)
n.datasets <- length(input.data)

input.species <- c()
subsample_factors <- c()
input.files <- c()

for (i in 1:length(input.data)){
  
  input.list <- input.data[[i]]
  # , "species", "cluster.resolution" , species and cluster.resolution
  if (!all(c("file") %in% names(input.list))){
    stop("file must be specied for each input dataset")
  }
  
  if (!(c("subsample") %in% names(input.list))) input.list$subsample <- 1
  if (!(c("integrate.by") %in% names(input.list))) input.list$integrate.by <- NA
  if (!(c("grouping.field") %in% names(input.list))) input.list$grouping.field <- NA
  if (!(c("groups") %in% names(input.list))) input.list$groups <- NA
  
  input.data[[i]] <- input.list

}

```


```{r analysis log}
message("Updating analysis log...")
# generate analysis log
df.log <- initiateLog("2, Data Integration")
df.log <- addLogEntry("Number of datasets", n.datasets, df.log, "n.datasets")
df.log <- addLogEntry("Subsample Factor", (as.vector(unlist(lapply(input.data, function(x) x$subsample)))), df.log, "")
df.log <- addLogEntry("Integrated Data (.Rdata)", ((as.vector(unlist(lapply(input.data, function(x) x$file))))), df.log, "input.data")
df.log <- addLogEntry("Integrated by", ((as.vector(unlist(lapply(input.data, function(x) x$integrate.by))))), df.log, "input.data")
df.log <- addLogEntry("Data Labels", (input.labels), df.log, "input.labels")
df.log <- addLogEntry("K Filter", (parameter.list$integration.k.filter), df.log, "integration.k.filter")
df.log <- addLogEntry("N Integration Genes", (parameter.list$integration.n.genes), df.log, "integration.n.genes")
df.log <- addLogEntry("N workers", (paste(names(parameter.list$n.workers), "=", 
                                          (purrr::map_dbl(parameter.list$n.workers,  1)), "workers",  collapse = "; ")), df.log, "n.workers")
df.log <- addLogEntry("max memory (Gb)",parameter.list$integration.rerun.SCT, df.log, "integration.rerun.SCT")
df.log <- addLogEntry("max memory (Gb)",parameter.list$integration.cluster.resolution, df.log, "integration.cluster.resolution")
df.log <- addLogEntry("max memory (Gb)",parameter.list$integration.limit.memory, df.log, "integration.limit.memory")
df.log <- addLogEntry("max memory (Gb)",parameter.list$integration.max.memory, df.log, "integration.max.memory")

```


```{r subsetting helper function}

preIntegrationSubsetting <- function(so, subset.df){

rna.assay <- so@assays[["RNA"]]
a.meta <- rna.assay@meta.features

if ("SCT" %in% names(so@assays)){
  v.meta <- so@assays[["SCT"]]@meta.features
  v.assay <- "SCT"
} else {
  v.meta <- so@assays[[DefaultAssay(so)]]@meta.features
  v.assay <- DefaultAssay(so)
}


gene.rep <- checkGeneRep(gNames.list, rownames(so))
if (is.null(rna.assay@counts@Dimnames[[1]])){
  if (gene.rep == "symbol"){
    rna.assay@data@Dimnames[[1]] <- rna.assay@counts@Dimnames[[1]] <- a.meta$SYMBOL
  } else if (gene.rep == "ensembl"){
    rna.assay@data@Dimnames[[1]] <- rna.assay@counts@Dimnames[[1]] <- a.meta$ENSEMBL
  }
}

so@assays[["RNA"]] <- rna.assay
rm (rna.assay)

# check if subset input is validd
if (is.na(unique(subset.df$field))){
  subset.flag <- FALSE
} else if ( unique(subset.df$field) %in% names(so@meta.data)) {
  subset.flag <- TRUE
} else {
  subset.flag <- FALSE
}

# subset data
if (subset.flag){
  
      cur.field <- as.vector(unique(subset.df$field))
  
  match.ind <- rep(F, length(as.character(so@meta.data[[cur.field]])))
  for (i in 1:length(subset.df$subgroups)){
    pattern <- as.vector(subset.df$subgroups)[i]
    pattern <- gsub(" ", "", pattern)

    match.ind <- (match.ind | grepl(pattern, as.character(so@meta.data[[cur.field]]), fixed = T))
  }
  
  so <- subset(x = so, cells = colnames(so)[which(match.ind)])
  so <- UpdateSeuratObject(so)
}

so@assays[["RNA"]]@meta.features <- a.meta
so@assays[[v.assay]]@meta.features <- v.meta


return(so)

}


```



```{r load data v2}

if (!exists("data.path") & !exists("user")) {
  stop("data.path and user variables do not exist in global enviroment. Ensure that these are specified in .Rprofile. If specified, try restarting Rstudio.")
}
message("Importing data...")

# get input file paths
input.files <- (as.vector(unlist(lapply(input.data, function(x) x$file))))
input.files <- paste0(data.path, dir, input.files)

# check if files exist
if (!all(file.exists(input.files))) stop("Not all input files exist\n")

# initiate lists
import.list <- list()

# start cluster
cl <- parallel::makeCluster(if (parameter.list$n.workers$import > n.datasets) n.datasets else parameter.list$n.workers$import)
doParallel::registerDoParallel(cl)



# iterate through each input file
import.list <- foreach(i = 1:length(input.files), .packages = c("scMiko", "Seurat"))  %dopar% {
# for (i in 1:length(input.files)){
  
  # get dataset name
  input.label <- input.labels[i]
  
  # ensure correct extension is present
  if (!grepl(".Rdata|.RData", input.files[i])) input.files[i] <- paste0(input.files[i], ".Rdata")
  
  # load and prep data
  load(input.files[i])
  so <- prepSeurat(so)
  
   try({
    so <- UpdateSCTAssays(so) # required after Seurat 3.1.2 update
  }, silent = T)
  
  # set correct cluster resolution
  # so <- setResolution(so, input.data[[input.label]]$cluster.resolution)
  
  # assign batch name
  so@meta.data[["batch"]] <- input.label
  
  # subset data (if subgroup specified)
  if (!is.na(input.data[[input.label]]$grouping.field)){
    subsetting.df <- data.frame(field = input.data[[input.label]]$grouping.field,
                                subgroups = input.data[[input.label]]$groups)
    so <- preIntegrationSubsetting(so, subsetting.df)
    
  } else {
    
    # ensure the dimensions are correctly defined in RNA assay (sometimes they are not?)
    a.meta <- so@assays[["RNA"]]@meta.features
    
    gene.rep <- checkGeneRep(gNames.list, rownames(so))
    if (is.null(so@assays[["RNA"]]@counts@Dimnames[[1]])){
      if (gene.rep == "symbol"){
        so@assays[["RNA"]]@counts@Dimnames[[1]] <- so@assays[["RNA"]]@data@Dimnames[[1]] <- a.meta$SYMBOL
      } else if (gene.rep == "ensembl"){
        so@assays[["RNA"]]@counts@Dimnames[[1]] <- so@assays[["RNA"]]@data@Dimnames[[1]]  <- a.meta$ENSEMBL
      }
    }
    
  }
  
  # subsample 
  if (input.data[[input.label]]$subsample < 1)  so <- downsampleSeurat(so, input.data[[input.label]]$subsample)
  
  return(list(so = so,
       gList = gNames.list,
       gNames = names(gNames.list),
       species = detectSpecies(so),
       input.label = input.label
  ))
}

# stop workers
parallel::stopCluster(cl)

# unpack results
so.list <- list()
gList <- list()
gNames <- list()

for (i in 1:length(import.list)){
  input.label <- import.list[[i]]$input.label
  so.list[[input.label]] <- import.list[[i]]$so
  gList[[input.label]] <- import.list[[i]]$gList
  gNames[[input.label]] <- import.list[[i]]$gNames
  input.data[[input.label]][["species"]] <- import.list[[i]]$species
}

# remove excess baggage
rm(import.list)


```


```{r data split, warning = FALSE}

# split seurat object by 'integrate.by' variable
input.data2 <- input.data
for (i in 1:length(input.data2)){
  
  set.name <- names(input.data2)[i]
  
  if (is.na(input.data2[[set.name]]$integrate.by)) next
  
  
  integrate.by <- input.data2[[set.name]]$integrate.by
  message(paste0("Splitting '", set.name, "' by '", integrate.by, "'..."))
  input.data.current <- input.data2[[set.name]]
  gList.current <- gList[[set.name]]
  gNames.current <-  gNames[[set.name]]
  
  if (!(integrate.by %in% colnames(so.list[[set.name]]@meta.data))){
    message(paste0("'", integrate.by , "' not found in '", set.name, "' dataset..."))
    next
  }
  
  # split object
  so.split <- Seurat::SplitObject(object = so.list[[set.name]], split.by = integrate.by)
  names(so.split) <- paste0(set.name, "_", names(so.split))
  
  
  new.input.data.list <- list()
  new.gList <- list()
  new.gNames <- list()
  for (j in 1:length(so.split)){
    new.input.data.list[[names(so.split)[j]]] <- input.data.current
    new.gList[[names(so.split)[j]]] <- gList.current
     new.gNames[[names(so.split)[j]]] <- gNames.current
     so.split[[j]]@meta.data[["batch"]] <- names(so.split)[j]
  }
  

  # update so.list
  so.list[[set.name]] <- input.data[[set.name]] <- gList[[set.name]] <- gNames[[set.name]] <- NULL
  so.list <- c(so.list, so.split)
  input.data <- c(input.data, new.input.data.list)
  gList <- c(gList, new.gList)
  gNames <- c(gNames, new.gNames)
  rm(so.split)
  invisible({gc()})
  
}

n.datasets <- length(so.list)
```


```{r rerun SCT}

if (parameter.list$integration.rerun.SCT){
  
  message("Running SCTransform normalization...")
  
  pmt.present <- all(unlist(lapply(X = so.list, FUN = function(x) "percent.mt" %in% colnames(x@meta.data))))
  if (pmt.present){
    var2reg <- "percent.mt"
  } else {
    var2reg <- NULL
  }
  
  so.list <- lapply(X = so.list, FUN = SCTransform, method = "glmGamPoi", verbose = F, 
                    vars.to.regress = var2reg, variable.features.n = parameter.list$integration.n.genes)
  
}

```



```{r subset common set of genes, fig.width = 12, fig.height=5}

message("Subsetting common genes...")

gene.av.list <- list()

gList.consolidate <- NULL

gene.tally.list <- list()

for (i in 1:length(so.list)){
  
  set.name <- names(so.list)[i]
  which.species <- input.data[[set.name]]$species
  
  if (which.species == "Mm") {
    ens.prefix <- "ENSMUSG"
  } else if (which.species == "Hs") {
    ens.prefix <- "ENSG"
  } else {
    stop("Species must be specified as 'Mm' or 'Hs'")
  }
  
  gene.rep <- rownames(so.list[[set.name]])

  which.match <- grepl(ens.prefix, gene.rep)
  
  if ((!(sum(which.match) == length(gene.rep))) & (checkGeneRep(gList[[set.name]], gene.rep) != "symbol")) {
    so.list[[set.name]] <- subset(so.list[[set.name]], features = gene.rep[which.match])
  }
  
  
  gene.tally.list[[set.name]] <-   gene.rep[which.match]
  
  so.list[[set.name]] <- ens2sym.so(object = so.list[[set.name]], gNames.list = gList[[set.name]])
  
  gList.consolidate <- bind_rows(gList.consolidate, data.frame(ens = gNames[[set.name]], sym = gList[[set.name]]))
  
  gene.av.list[[set.name]] <-  rownames(so.list[[set.name]])
  
}

# gene availability tally
av.tally <- data.frame(table(unlist(gene.av.list)))
colnames(av.tally) <- c("Gene", "N")

# visualize availability
plt.gene.av.tally <- av.tally %>%
  dplyr::filter(N <= length(so.list)) %>%
  ggplot(aes(x = N)) + 
  geom_histogram(bins =  length(so.list), color = "black", fill = "grey") + 
  geom_text(stat = 'count',aes(label =..count.., vjust = -0.2)) + 
  theme_miko() + 
  xlab( paste0("Gene Representation (","/",as.integer(length(so.list)), " datasets)" )) + 
  ylab("Gene Count") + 
  labs(title = "Gene representation across datasets", subtitle = "Histogram")

# get intersecting set of genes
gene.intersect <- Reduce(intersect, gene.av.list)

# get common genes
df.common.genes <- NULL
for (i in 1:length(gene.av.list)){
    set.name <- names(gene.av.list)[i]
  current.list <- gene.av.list[[set.name]]

  
  for (j in 1:length(so.list)){
    current.tally <- as.character(av.tally$Gene[av.tally$N == j])
    df.common.genes <- bind_rows(df.common.genes, 
                                 data.frame(set = set.name,
                                            rep = j,
                                            n = sum(current.list %in%current.tally )
                                   
                                 ))
  }
}

df.common.genes$rep <- factor(df.common.genes$rep, levels = seq(1, length(gene.av.list)))
plt.common.bar <- df.common.genes %>%
  ggplot(aes(x = set, y = n, fill = rep)) + 
  geom_bar(stat = "identity") + 
  theme_miko(legend = T, x.axis.rotation = 45) + 
  xlab("Dataset") + ylab( paste0("Gene Count" )) + 
  labs(fill = paste0("Gene Rep.\n(","/",as.integer(length(so.list)), " datasets)" ),
      subtitle = "Stacked counts") + 
  scale_fill_manual(values = categoricalColPal(n = length(so.list)))
  # ggthemes::scale_fill_gdocs()

# convert list to df
gene.av.df <- namedList2wideDF(gene.av.list)

# merge plots
plt.common.gene.plot <- cowplot::plot_grid(plt.gene.av.tally, plt.common.bar, ncol = 2, align = "hv", rel_widths = c(1.5, 2))
if (parameter.list$print.inline){
  print(plt.common.gene.plot)
}


for (i in 1:length(so.list)){
  set.name <- names(so.list)[i]
  so.list[[set.name]] <- subset(so.list[[set.name]], features = gene.intersect)
}

# generate master ens-sym mapping list
rownames(gList.consolidate) <- NULL
gList.cu <- unique(gList.consolidate)
gList.cu <- gList.cu[!duplicated(gList.cu$sym), ]
gList.cu <- gList.cu[!duplicated(gList.cu$ens), ]
gNames.list <- gList.cu$sym
names(gNames.list) <- gList.cu$ens





```


```{r get original (input) umaps, fig.width=5, fig.height=7}

# get original umap embedding and df meta

df.umap.orig <- NULL
df.meta.orig <- NULL
for (i in 1:length(so.list)){
  
  try({
    batch.name <- names(so.list)[i]
    df.umap.orig <- bind_rows(df.umap.orig, data.frame(
      batch = batch.name,
      umap.x = so.list[[batch.name]]@reductions[["umap"]]@cell.embeddings[ ,1],
      umap.y = so.list[[batch.name]]@reductions[["umap"]]@cell.embeddings[ ,2]
    ))
  }, silent = T)
  
}

set.labels <- unique(df.umap.orig$batch)
plt.original_overlay <- list()
for (i in 1:length(set.labels)){
  
  df.umap.orig.cur <- df.umap.orig
  df.umap.orig.cur$is.in <- df.umap.orig.cur$batch %in% set.labels[i]
  plt.original_overlay[[set.labels[i]]] <- df.umap.orig.cur %>%
    dplyr::arrange(is.in) %>%
    ggplot(aes(x = umap.x, y = umap.y, color = is.in)) + 
    geom_point(size = autoPointSize(nrow(df.umap.orig.cur))) + 
    scale_color_manual(values = c("TRUE" = "red", "FALSE" = "grey")) +   
    ggtitle(label = set.labels[i]) + 
    xlab("UMAP 1") +
    ylab("UMAP 2") + 
    theme_miko()
}


plt.umap.original <- df.umap.orig %>%
  ggplot(aes(x= umap.x, y = umap.y, color = batch)) + 
  geom_point(size = autoPointSize(nrow(df.umap.orig))) + 
  theme_miko(legend = T) + 
  labs(title = "Pre-Integration Data", subtitle = "UMAP") + 
   scale_fill_manual(values = categoricalColPal(n = length(so.list))) + 
  xlab("UMAP 1") + ylab("UMAP 2") + 
  theme_miko(legend = T)  + 
  theme(legend.position = "bottom")  +
  guides(color = guide_legend(override.aes = list(size = 5))) + 
   theme(legend.text=element_text(size=rel(0.5))) 
  # guide_legend(size = 2)

if (parameter.list$print.inline){
  plt.umap.original
}
```



```{r relative distribution of input data}

cell_per_set <- purrr::map_dbl(so.list, ncol)

cps.df <- data.frame(batch = names(cell_per_set), n = cell_per_set)

cps.df <- cps.df %>% 
  dplyr::arrange(desc(batch)) %>%
  dplyr::mutate(prop = n / sum(cps.df$n) *100) %>%
  dplyr::mutate(ypos = cumsum(prop)- 0.5*prop )


# Basic piechart
plt.pie <- ggplot(cps.df, aes(x="", y=prop, fill=batch)) +
  geom_bar(stat="identity", width=1, color="white") +
  coord_polar("y", start=0) +
  theme_void() + 
  # geom_text(aes(y = ypos, label = n), color = "white", size=2) +
   scale_fill_manual(values = categoricalColPal(n = length(so.list))) + 
  labs(title = "Cell Frequency Per Dataset", subtitle = "Piechart")


# if (length(so.list) > 10){
  plt.pie <- plt.pie   +
  guides(fill = guide_legend(override.aes = list(size = 5))) + 
   theme(legend.text=element_text(size=rel(0.5))) 
    # theme(legend.position = "none")
# }

if (parameter.list$print.inline){
  plt.pie
}

```


```{r ensure cell names are unique}

# Helper function
appendCellPrefix <- function(so, prefix){
  
  if (class(so) != "Seurat") stop("Data must be Seurat Object")
  
  which.assay <- DefaultAssay(so)
  
  # store original names
  so@meta.data[["orig_cellname"]] <- colnames(so)
  
  # update cell names
  rownames(so@meta.data) <-  paste0(cell.prefix, rownames(so@meta.data))
  names(so@active.ident) <- paste0(cell.prefix,names(so@active.ident))
  so@assays[[which.assay]]@misc[["vst.out"]][["cells_step1"]] <- paste0(cell.prefix, so@assays[[which.assay]]@misc[["vst.out"]][["cells_step1"]])
  
  rownames(so@assays[[which.assay]]@misc[["vst.out"]][["cell_attr"]]) <- paste0(cell.prefix, rownames(so@assays[[which.assay]]@misc[["vst.out"]][["cell_attr"]]))
  
  
  if (!is.null(so@assays[[which.assay]]@misc[["vst.out"]][["umi_corrected"]])){
    so@assays[[which.assay]]@misc[["vst.out"]][["umi_corrected"]]@Dimnames[[2]] <- paste0(cell.prefix,  so@assays[[which.assay]]@misc[["vst.out"]][["umi_corrected"]]@Dimnames[[2]])
  }
  
 colnames(so@assays[[which.assay]]@scale.data) <- paste0(cell.prefix,colnames(so@assays[[which.assay]]@scale.data))
  
  
  for (k in 1:length(names(so@assays))){
    assay.name <- names(so@assays)[k]
    so@assays[[assay.name]]@counts@Dimnames[[2]] <- paste0(cell.prefix, so@assays[[assay.name]]@counts@Dimnames[[2]])
    so@assays[[assay.name]]@data@Dimnames[[2]] <- paste0(cell.prefix, so@assays[[assay.name]]@data@Dimnames[[2]])
  }
  
  for (k in 1:length(names(so@reductions))){
    red.name <- names(so@reductions)[k]
    rownames(so@reductions[[red.name]]@cell.embeddings) <- paste0(cell.prefix,rownames(so@reductions[[red.name]]@cell.embeddings))
  }
  
  return(so)
  
}


all.cell.names <- c()
for (i in 1:length(so.list)){

  # define prefix
  cell.prefix <- paste0("S", i, "_")

  # append to existing dataset (to ensure unique cell names prior to integration)
  try({so.list[[i]] <- appendCellPrefix(so.list[[i]], cell.prefix)}, silent = T)
  
  all.cell.names <- c(all.cell.names, colnames(so.list[[i]]))

}

# ensure cell names are unique
stopifnot(length(all.cell.names) == length(unique(all.cell.names)))

```



```{r update variable features, include = FALSE}

# integration.n.genes <- 2000
# integration.n.genes2 <- round(1.5 * integration.n.genes)

for (i in 1:length(so.list)){
  
  if (!("SCT" %in% names(so.list[[i]]@assays))) stop("All objects must have 'SCT' assay. Other normalization pipelines are not supported.")
  if (class(so.list[[i]]@assays[["SCT"]]) == "SCTAssay"){
    df.var <-  so.list[[i]]@assays[["SCT"]]@SCTModel.list[["model1"]]@feature.attributes
    if (!("SYMBOL" %in% colnames(df.var))){
      
      if (!("ENSEMBL" %in% colnames(df.var))){
        df.var$key <- stringr::str_extract( rownames(df.var),"[0-9A-Za-z-]*")
      }
      # df.mapping <-  ensembl2sym(rownames(df.var), detectSpecies(so.list[[i]]))
      e2s.success <- F
      try({
        df.mapping <-  ensembl2sym(df.var$key, detectSpecies(so.list[[i]]))
        merge.by <- "ENSEMBL"
        df.var$ENSEMBL <- df.var$key
        e2s.success <- T
      }, silent = T)
      s2e.success <- F
      try({
        df.mapping <-  sym2ens(df.var$key, detectSpecies(so.list[[i]]))
        merge.by <- "SYMBOL"
        df.var$SYMBOL <- df.var$key
        s2e.success <- T
      }, silent = T)
      
      
      df.var <- merge(df.var, df.mapping, by = merge.by)
      
    }
    df.var <- df.var[!is.na(df.var$SYMBOL), ]
    so.list[[i]]@assays[["SCT"]]@SCTModel.list[["model1"]]@feature.attributes <- df.var
    df.var <- df.var %>% dplyr::arrange(-residual_variance)
    so.list[[i]]@assays[["SCT"]]@var.features <- df.var$SYMBOL[1:round(1.5 * parameter.list$integration.n.genes)]   
  } else {
    df.var <-  so.list[[i]]@assays[["SCT"]]@meta.features 
    df.var <- df.var %>% dplyr::arrange(-sct.residual_variance)
    so.list[[i]]@assays[["SCT"]]@var.features <- df.var$SYMBOL[1:round(1.5 * parameter.list$integration.n.genes)]    
  }
  
  
}

```


```{r integrate data, include = FALSE}

       # 

if (length(so.list) > 1){ # intergrate if there are multiple datasets provided
  
  # Select features for downstream integration 
  assay.vector <- c()
  for (i in 1:n.datasets) {
    assay.vector[i] <- DefaultAssay(so.list[[i]])
     Idents(so.list[[i]]) <- so.list[[i]]@meta.data[["seurat_clusters"]]
    so.list[[i]]@meta.data[["orig.ident"]] <-Idents(so.list[[i]])
  }
  
  # enable parallelization
    plan(strategy = "multisession", workers = parameter.list$n.workers$integration)
    if (parameter.list$integration.limit.memory) options(future.globals.maxSize = (parameter.list$integration.max.memory*20480/20 * 1024^2))    

message("Selecting integration features...")
  so.features <- SelectIntegrationFeatures(object.list = so.list,
                                           assay = assay.vector,
                                           nfeatures = parameter.list$integration.n.genes,
                                           fvf.nfeatures = parameter.list$integration.n.genes)
  
  # prepare seurat object for integration
  message("Prepping seurat object for integration..")
  
  # integration.k.filter <- 200
  so.n <- unlist(lapply(so.list, ncol))
  which.too.small <- names(so.n)[so.n <= parameter.list$integration.k.filter]
  if (length(which.too.small) > 0){
    message(paste0(which.too.small, collapse = ", "), " were removed due to insufficient number of cells (n < ", parameter.list$integration.k.filter, ")")
    n.datasets <- sum(so.n > parameter.list$integration.k.filter)
  }
  
  
  so.list2 <- tryCatch({
    PrepSCTIntegration(object.list = so.list[so.n > parameter.list$integration.k.filter ], anchor.features = so.features, verbose = T)
  }, error = function(e){
           pmt.present <- all(unlist(lapply(X = so.list, FUN = function(x) "percent.mt" %in% colnames(x@meta.data))))
       if (pmt.present){
         var2reg <- "percent.mt"
       } else {
         var2reg <- NULL
       }
       
    so.list <- lapply(X = so.list[so.n > parameter.list$integration.k.filter ], FUN = SCTransform, method = "glmGamPoi", verbose = F, 
                       vars.to.regress = var2reg, variable.features.n = parameter.list$integration.n.genes)

    so.features <- SelectIntegrationFeatures(object.list = so.list,
                                           assay = assay.vector,
                                           nfeatures = parameter.list$integration.n.genes,
                                           fvf.nfeatures = parameter.list$integration.n.genes)
     
    so.list2 <- PrepSCTIntegration(object.list = so.list, anchor.features = so.features, verbose = T)
    return(list(
      so.list = so.list2,
      so.features = so.features,
      unpack.this = T
    ))
  })
  
  if ("unpack.this" %in% names(so.list2)){
    so.features <- so.list2$so.features
    so.list2 <- so.list2$so.list
  }
  
  
  #########
  #########
  
  # Added override step
  # Note: solution to retain sct scaled values post-integration proposed by chlee-tabin (https://github.com/satijalab/seurat/issues/2590)
  retain.sct.scale <- F
  if (retain.sct.scale){
    for (i in 1:n.datasets) {
      so.list2[[i]][["SCT"]] <- SetAssayData( 
        object = so.list2[[i]][["SCT"]], 
        slot = "scale.data", 
        new.data = GetAssayData( so.list[[i]], assay = "SCT", slot = "scale.data" )
      )
    }    
  } 
  rm(so.list); invisible({gc()})
  
  # ensure the k.filer is not larger than smallest dataset
  # integration.k.filter <- min(200, min(sapply(so.list2, ncol)))

  # # find integration anchors
  message("Finding integration anchors...")
  suppressMessages({
    suppressWarnings({
      
      if (parameter.list$integration.method == "CCA") {
        so.anchors <- FindIntegrationAnchors(object.list = so.list2, normalization.method = "SCT",
                                             anchor.features = so.features, verbose = FALSE, k.filter = parameter.list$integration.k.filter)
      }else if (parameter.list$integration.method == "rPCA") {
        
        # run PCA on each object in the list (required for reicprocal PCA workflow)
        so.list2 <- lapply(X = so.list2, FUN = function(x) {
          x <- RunPCA(x, features = so.features, verbose = FALSE)
        })
        # identify anchors and integrate datasets
        so.anchors <- FindIntegrationAnchors(object.list = so.list2, normalization.method = "SCT", reduction = "rpca",
                                             anchor.features = so.features, verbose = FALSE)
        
      }
    })
  })
  
  rm(so.list2); invisible({gc()})

  # integrate data
  message("Integrating data...")
  so <- IntegrateData( 
    anchorset = so.anchors, 
    normalization.method = "SCT",
    features.to.integrate = NULL, # which.features.integrate
    verbose = T
  )
  
  # Switch to integrated assay. Variable features are automatically set during IntegrateData()
  DefaultAssay(so) <- "integrated"
} else {
  so <- so.list[[1]]
  rm(so.list)
}

# integration.n.genes.effect <- length(which.features.integrate)
n.anchors.effect <- parameter.list$integration.n.genes

# remove baggage
rm(so.anchors);invisible({gc()})


```




```{r PCA}

# Run PCA
message("Running PCA...")
so <- RunPCA(so, verbose = FALSE)

# proportion of variance explained by each PC 
pc.std <- so@reductions[["pca"]]@stdev
pc.var <- pc.std^2
pc.prop_var <- pc.var/sum(pc.var)
pc.cum_sum <- cumsum(pc.prop_var)
pc.id <- c(1:50)
scree.var <- data.frame(pc.id, pc.prop_var, pc.cum_sum)

# Number of dims to use
pca.var.threshold <- 0.9
pca.prop <- propVarPCA(so)
pc.n_relevant_components <- max(pca.prop$pc.id[pca.prop$pc.cum_sum<pca.var.threshold])+1
#pc.n_relevant_components <- 30

# generate Scree Plot
plt.scree1 <- ggplot(scree.var, aes(x = pc.id, y = pc.prop_var)) + 
  geom_point() +  
  theme(legend.position="right") +
  geom_vline(xintercept = pc.n_relevant_components+0.5, color = 'red') + 
  ggtitle("Scree Plot") + 
  xlab("Principal Components") + 
  ylab("Variance Explained (proportion)") + 
  theme_miko()

plt.scree2 <- ggplot(scree.var, aes(x = pc.id, y = pc.cum_sum)) + 
  theme(legend.position="right") +
  geom_vline(xintercept = pc.n_relevant_components+0.5, color = 'red') + 
  geom_hline(yintercept =pca.var.threshold, color = 'red') + 
    geom_point() +
  ylim(0, 1) + 
  ggtitle(paste(pc.n_relevant_components, "PCs (", round(100*pc.cum_sum[pc.n_relevant_components],2), "%) of variance")) + 
  xlab("Principal Components") + 
  ylab("Cumulative Variance Explained (proportion)") + theme_miko()


if (parameter.list$print.inline){
  print(plt.scree1)
  print(plt.scree2)
}

```


```{r cluster data}
# Find clusters ---------------------------------------------------------------------------
message("Finding nearest neighbors...")
so <- FindNeighbors(object = so, reduction = "pca", dims = 1:pc.n_relevant_components)

# integration.cluster.resolution <- 1
message("Clustering data...")
so <- FindClusters(object = so, resolution = parameter.list$integration.cluster.resolution, 
                   verbose = 0, algorithm = 1, modularity.fxn = 1, group.singletons = T)

```

```{r run_umap}

# Run umap and generate plot --------------------------------------------------------------------
nDim_umap <- pc.n_relevant_components
so <- RunUMAP(so, dims = 1:nDim_umap)
plt.integration <- DimPlot(so, reduction = "umap",group.by = "batch", pt.size = autoPointSize(ncol(so)))  + ggtitle(label = "UMAP") +
  xlab("UMAP 1") + ylab("UMAP 2")

plt.integration <-  plt.integration + 
  theme_miko(legend = T) + 
  labs(title = "Integrated Data", subtitle = "UMAP") + 
   scale_fill_manual(values = categoricalColPal(n = n.datasets)) + 
  theme(legend.position = "bottom") +
  guides(color = guide_legend(override.aes = list(size = 5))) + 
   theme(legend.text=element_text(size=rel(0.5))) 


if (parameter.list$print.inline){
  print(plt.integration)
}

```


```{r umap by pilot, fig.width=16, fig.height=15}

plt.integration_overlay <- list()
# show umap stratified by batch


set.labels <- unique(so@meta.data[["batch"]])
for (i in 1:length(set.labels)){
  plt.integration_overlay[[set.labels[i]]] <- DimPlot(so, 
                                          reduction = "umap", 
                                          label = F, cells.highlight = as.vector(which(so@meta.data[["batch"]] == set.labels[i])), 
                                          label.size = 4,
                                          pt.size = autoPointSize(ncol(so)),
                                          sizes.highlight = autoPointSize(ncol(so)))  +   
    ggtitle(label = set.labels[i]) + 
    xlab("UMAP 1") +
    ylab("UMAP 2") + 
    theme_miko()
}


plt.overlay.combine <- list()
for (i in 1:length(plt.original_overlay)){
  
  set.name <- names(plt.original_overlay)[i]
  
  plt.overlay.combine[[set.name]] <- cowplot::plot_grid(plt.original_overlay[[set.name]] + labs(subtitle = "Pre-integration"), 
                                                        plt.integration_overlay[[set.name]] + labs(subtitle = "Post-integration", title = ""), nrow = 1)
  
}

plt.ncol <- if (n.datasets > 3) 4 else  n.datasets
plt.integration_by_batch <- cowplot::plot_grid(plotlist = plt.integration_overlay, ncol = plt.ncol)

if (parameter.list$print.inline){
  print(plt.integration_by_batch)
}

# plt.integration_overlay

```

```{r umap by cluster, warning=FALSE}


# show umap stratified by cluster
plt.umap_by_cluster <- DimPlot(so, 
                               reduction = "umap",  
                               label.size = 4, 
                               label = TRUE, 
                               pt.size = autoPointSize(ncol(so)))  +   
  ggtitle("Clusters") + 
  xlab("UMAP 1") + 
  ylab("UMAP 2") 

plt.umap_by_cluster <- plt.umap_by_cluster + 
        theme_miko(legend = T) + 
        labs(title = "Integrated Dataset", subtitle = "UMAP stratfiied by clusters") + 
  scale_color_manual(values = categoricalColPal(n = length(unique(so@meta.data$seurat_clusters))))  

if (parameter.list$print.inline){
  print(plt.umap_by_cluster)
}


```



```{r umap by barcode, fig.height=5, fig.width=10}

plt.umap_by_barcode <- DimPlot(so, split.by = "batch", group.by = "Barcode",  pt.size = 0.02) + 
  xlab("UMAP 1") + ylab("UMAP 2")+ theme(legend.position = "bottom")

plt.umap_by_batch <- DimPlot(so, group.by = "batch",  pt.size = 0.02) + 
  xlab("UMAP 1") + ylab("UMAP 2")  +
  guides(color = guide_legend(override.aes = list(size = 5))) + 
   theme(legend.text=element_text(size=rel(0.5))) 

if (parameter.list$print.inline){
  print(plt.umap_by_barcode)
  plt.umap_by_batch
}


```


```{r function for long to wide annotation table}

long2wide <- function(df){
  
  # create wide version (for output to excel), reorder factors, and get basic descriptive stats
  df.wide <- dcast(df, cluster ~ batch, value.var = "n")
  df.wide[is.na(df.wide)] <- 0
  reordered_factors <- order(as.numeric(as.vector(df.wide$cluster)))
  df.wide <- df.wide[reordered_factors, ]
  rownames(df.wide) <- df.wide$cluster
  df.wide <- as.data.frame(t(df.wide))
  df.wide <- df.wide[2:dim(df.wide)[1], ]
  df_id_rnames <- rownames(df.wide)
  colnames(df.wide) <- paste("cluster", colnames(df.wide), sep = "")
  df.wide <- data.frame(apply(df.wide, 2, function(x) as.numeric(as.character(x))))
  df.wide[dim(df.wide)[1]+1, ] <- apply(df.wide, 2, sum)
  rownames(df.wide) <- c(df_id_rnames, "TOTAL")
  df.wide[, dim(df.wide)[2]+1] <- apply(df.wide, 1, sum)
  colnames(df.wide)[dim(df.wide)[2]] <- "TOTAL"
  
  return(df.wide)
}

```

```{r compare cluster composition}


batches <- as.vector(so@meta.data[["batch"]])
clusters <- as.vector(so@meta.data[["seurat_clusters"]])

u.batches <- unique(batches)
u.clusters <- unique(clusters)

df.cluster_comp <- data.frame(batch = batches, 
                              cluster = clusters)


df.tally <- df.cluster_comp %>%
  group_by(cluster, batch) %>%
  tally() %>%
  mutate(freq = n/sum(n))

df_for_wide <- df.tally
# df.all_id_wide <- long2wide(df_for_wide)

df.cluster_annotations <- df.tally

u.batches <- unique(batches)
n.batches <- length(u.batches)

color_count <- max(n.batches)
my_cols = colorRampPalette(brewer.pal(color_count, "Set2"))(color_count)

# ensure that clusters are ordered numerically
reordered_clusters <- order(as.numeric(as.vector(df.cluster_annotations$cluster)))
df.cluster_annotations <- df.cluster_annotations[reordered_clusters, ]
df.cluster_annotations$cluster <- as.numeric(as.vector(df.cluster_annotations$cluster))
cluster_chart_labels <- unique(df.cluster_annotations$cluster)

# df.cluster_annotations_int <- df.cluster_annotations_int[order(levels(df.cluster_annotations_int$cluster_membership_int)), ]
plt.cluster_composition <- ggplot(df.cluster_annotations, 
                                  aes(x = cluster, 
                                      fill = batch, 
                                      y = freq)) +
  geom_bar(position = "fill", stat = "identity") + 
  scale_x_continuous("Cluster", labels = as.character(cluster_chart_labels), breaks = cluster_chart_labels) + 
  xlab("Cluster ID") + ylab("Cluster Representation") + 
  ggtitle("Cluster Composition") + 
  scale_fill_manual(values = categoricalColPal(n = ulength(df.cluster_annotations$batch))) + 
  # ggthemes::scale_fill_ptol() + 
  theme_miko(legend = T) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))


try({
 plt.cluster_composition <- plt.cluster_composition +
   theme(legend.position = "bottom") + 
  guides(fill = guide_legend(override.aes = list(size = 5))) + 
   theme(legend.text=element_text(size=rel(0.5))) 
}, silent = T)
  # scale_fill_manual(values = my_cols) + 


if (parameter.list$print.inline){
  plt.cluster_composition
}


```


```{r # pre post integration values, fig.width = 12, fig.height = 7}
# 
# plt.bulk.comparison <- NULL
# try({
# 
# df.exp <- NULL
# 
# e.mat.integrated <- getExpressionMatrix(so, which.data = "scale", which.assay = "integrated")
# e.mat.sct <- getExpressionMatrix(so, which.data = "scale", which.assay = "SCT")
# 
# df.meta <- so@meta.data
# u.batch <- unique(df.meta$batch)
# n.batches <- length(u.batch)
# 
# for (i in 1:n.batches){
#   batch.name <- u.batch[i]
#   
#   df.meta.sub <- df.meta[df.meta$batch %in% batch.name, ]
#   
#   which.cells <-  rownames(df.meta.sub)
#   
#   ei.sub <- e.mat.integrated[ ,colnames(e.mat.integrated) %in% which.cells]
#   es.sub <- e.mat.sct[ ,colnames(e.mat.sct) %in% which.cells]
#   
#   # get means and sort according to genes
#   ei.mean <- rowMeans(ei.sub)
#   ei.mean <- ei.mean[order(names(ei.mean))]
#   es.mean <- rowMeans(es.sub)
#   es.mean <- es.mean[order(names(es.mean))]
#   es.mean <- es.mean[names(es.mean) %in% names(ei.mean)]
#   
#   df.exp.sub <- NULL
#   df.exp.sub <- data.frame(batch = batch.name, genes = names(es.mean), sct.expression = es.mean, integrated.expression = ei.mean)
#   
#   df.exp <- bind_rows(df.exp, df.exp.sub)
#   
# }
# 
# lm_eqn <- function(df){
#     m <- lm(y ~ x, df);
#     eq <- substitute(italic(y) == a + b %.% italic(x)*","~~italic(r)^2~"="~r2, 
#          list(a = format(unname(coef(m)[1]), digits = 2),
#               b = format(unname(coef(m)[2]), digits = 2),
#              r2 = format(summary(m)$r.squared, digits = 3)))
#     as.character(as.expression(eq));
# }
# 
# plt.bulk.comparison <- df.exp %>%
#   ggplot(aes(x = sct.expression, y = integrated.expression)) + 
#   geom_hline(yintercept = 0, color = "grey") + 
#    geom_vline(xintercept = 0, color = "grey") + 
#   geom_point(alpha = 0.5) + 
#   geom_smooth(method = "lm", color = "black", linetype = "dashed", se = FALSE) + 
#   ggpmisc::stat_poly_eq(formula = y~x, aes(label = paste(..rr.label.., sep = "~~~")), parse = T) + 
#   facet_wrap(~batch, ncol = 4, scales = "free") + 
#   theme_miko() + 
#   geom_abline(slope = 1, linetype = "dashed", color ="tomato") + 
#   labs(title = "Pseudobulk Expression", subtitle = "Pre- vs. Post-Integration Gene Expression") + 
#   xlab("SCT, Scaled Gene Expression") + 
#   ylab("Integrated, Scaled Gene Expression")
# 
# if (parameter.list$print.inline) {
#   print(plt.bulk.comparison)
# }
#   
#   
# })

```

```{r central log}

# update central log
run.id <- NULL
if (!exists("user")) user <- "guest"

clog.update.success <- F
if (parameter.list$update.log){
 try({
  run.id <-  updateCentralLog(Module = "M02", 
                              input.data = paste(unlist(lapply(input.data, function(x) x$file)), collapse = ", "), 
                              input.subset = NA, 
                              pdf.flag = parameter.list$save.pdf)
  clog.update.success <-  T
}, silent = F) 
}

if (is.null(run.id))  {
  message("Central log update was unsuccessful :(\n")
  run.id <- paste("M02_", user, "_r", paste0(format(Sys.time(), '%s')), sep = "", collapse = "")
}

```

```{r setup output directories}

# output path
if (!exists("data.path")) data.path = ""
output.path <- paste0(data.path, "Module_Outputs/", paste0(run.id,"_",format(Sys.time(), '%d%m%y'), "/"))

# create output directories
dir.create(output.path)
dir.create(paste0(output.path, "Tables/"))
if (parameter.list$save.pdf) dir.create(paste0(output.path, "PDF/"))

```

Data Integration
===================================== 

Sidebar {.sidebar}
-------------------------------------

**scRNAseq Data Integration**

**Description**: Create an integrated dataset for downstream scRNAseq analysis. 

**Method**: Cross-dataset cell pairs (i.e., anchors) are identified based on corresponding biological states and these anchors are used to harmonize datasets into a single dataset, thereby correcting for technical differences across datasets (i.e., batch effect correction). 

**Method**: Stuart, T., Butler, A., Hoffman, P., Hafemeister, C., Papalexi, E., Mauck III, W. M., ... & Satija, R. (2019). Comprehensive integration of single-cell data. *Cell*, 177(7), 1888-1902



Row {.tabset data-height=500}
-------------------------------------

### Integration

```{r plt.integration, fig.width=12, fig.height=7}

umap.oi <- cowplot::plot_grid(plt.umap.original , plt.integration, align = "v",  ncol = 2)
print(umap.oi)

try({
  savePDF(file.name = paste0(output.path, "PDF/", "M02_UMAP_input_vs_integrated.pdf"), 
          plot.handle = umap.oi, 
          fig.width = 12, fig.height = 7, save.flag = parameter.list$save.pdf)
}, silent = T)

```

### Number of Cells per Dataset
```{r n pie chart}
  plt.pie
```

```{r save pie}

try({
  savePDF(file.name = paste0(output.path, "PDF/", "M02_pie_composition.pdf"), 
          plot.handle = plt.pie, save.flag = parameter.list$save.pdf)
}, silent = T)

```


### PCA 
```{r plt.PCA, fig.width=8, fig.height=4, dpi = 72}

# PCA & Cluster Resolution
# ===================================== 
# PCA
# Row
# -----------------------------------------------------------------------
plt.scree.combo <- cowplot::plot_grid(plt.scree1, plt.scree2, ncol=2)
print(plt.scree.combo)

try({
  savePDF(file.name = paste0(output.path, "PDF/", "M02_scree_plot.pdf"), 
          plot.handle = plt.scree.combo, 
          fig.width = 8, fig.height = 4, save.flag = parameter.list$save.pdf)
}, silent = T)

```

### Integrated Clusters

```{r plt.cluster_composition, fig.width = 12, fig.height = 4}

plt.comp.combo <- cowplot::plot_grid(plt.umap_by_cluster + theme(legend.position = "none"), 
                                     plt.cluster_composition, rel_widths = c(1,1.5),nrow = 1,  axes = "tb")
print(plt.comp.combo)

try({
  savePDF(file.name = paste0(output.path, "PDF/", "M02_sample_composition.pdf"), 
          plot.handle = plt.comp.combo, 
          fig.width = 12, fig.height = 4, save.flag = parameter.list$save.pdf)
}, silent = T)

# Row
# -----------------------------------------------------------------------
# 
# ### Principal Components
# ```{r valuebox4}
# valueBox(pc.n_relevant_components)
# ```
# 
# ### Clusters
# ```{r valuebox5}
# valueBox(length(unique(so@meta.data[["seurat_clusters"]])))
# ```

```

### Gene Intersection
```{r n common geneplot, fig.width = 12, fig.height =5}
  print(plt.common.gene.plot)
```

```{r save common gene plot}

try({
  savePDF(file.name = paste0(output.path, "PDF/", "M02_gene_intersection.pdf"), 
          plot.handle = plt.common.gene.plot, fig.width = 12, fig.height =5, save.flag = parameter.list$save.pdf)
}, silent = T)

```



Row {.tabset data-height=500}
-------------------------------------

```{r sample-specific plots}

out <- lapply(seq_along(plt.overlay.combine), function(i) {
  
  s1 <- names(plt.overlay.combine)[i]
  s2 <- paste0("plt.overlay.combine[[", i, "]]")
  
  a1 <- knitr::knit_expand(text = sprintf("### %s\n", s1))  # tab header
  a2 <- knitr::knit_expand(text = sprintf("\n```{r %s, message=FALSE, warning=FALSE, fig.width = 14, fig.height=6}",  #fig.width = 8, fig.height=8, 
                                          paste("inOver_", i, sep = "")))
  a3 <- knitr::knit_expand(text = sprintf("\n %s", s2)) 
  a4 <- knitr::knit_expand(text = "\n```\n") # end r chunk
  
  paste(a1, a2, a3, a4, collapse = '\n') # collapse together all lines with newline separator
  
})

```

`r paste(knitr::knit(text = paste(out, collapse = '\n')))`

Row
-----------------------------------------------------------------------

### Datasets
```{r valuebox1}
valueBox(n.datasets)
```

### Cells
```{r valuebox1-2}
valueBox(ncol(so))
```

### Integrated Genes
```{r valuebox2}
valueBox(length(so.features))
```

### Total Genes
```{r valuebox3}
try({valueBox(nrow(so@assays[["RNA"]]))}, silent = T)
```






Tables
===================================== 

### Integrated Genes

Genes used for data integration.
```{r integ genes}

try({
 flex.asDT(data.frame(gene = so.features))
}, silent = T)

```

### Gene Intersection Tally
Tally of genes represented across datasets. 
```{r available genes}

try({
 flex.asDT(av.tally)
}, silent = T)

```

### Gene Intersection

Genes represented in all datasets. 
```{r gene int table}

try({
 flex.asDT(gene.av.df)
}, silent = T)

```


```{r tally table}

try({
  table.name <- "gene_intersection_tally_table.csv"
  write.csv(av.tally, file = paste0(output.path, "Tables/", table.name),
            row.names = F)
}, silent = T)

try({
  table.name <- "gene_intersection_table.csv"
  write.csv(gene.av.df, file = paste0(output.path, "Tables/", table.name),
            row.names = F)
}, silent = T)

```


```{r save results}

# Data Integration Method
if (parameter.list$integration.method == "CCA"){int_method <- "CCA"} else if (parameter.list$integration.method == "rPCA"){int_method <- "rPCA"}
df.log <- addLogEntry("Integration Method", parameter.list$integration.method, df.log, "integration.method")

end.time <- proc.time()
elapsed.time <- round((end.time - start.time)[[3]], 2)
df.log <- addLogEntry("Run Time (s)", elapsed.time, df.log, "elapsed.time")
df.log <- addLogEntry("Results Saved", parameter.list$save.integrated.object, df.log, "save.integrated.object")
df.log <- addLogEntry("PDFs Saved", parameter.list$save.pdf, df.log, "save.pdf")

df.log <- addLogEntry("Run Identifier", run.id, df.log, "run.id")
df.log <- addLogEntry("User", user, df.log, "user")
df.log <- addLogEntry("Central Log Updated", clog.update.success, df.log, "clog.update.success")

parameter.list$save.filename <- paste0(run.id, "_", parameter.list$save.filename)
df.log <- addLogEntry("Output File", (parameter.list$save.filename), df.log, "save.filename")

df.log_Module_2 <- df.log

if (parameter.list$save.integrated.object == TRUE){
  if (!exists("dir")){dir <- ""} 
  save(so, gNames.list, df.log_Module_2, file = paste0(data.path, dir, parameter.list$save.filename))
}

```


Log (Module 2)
===================================== 

```{r table.log_current}

knitr::kable(df.log_Module_2)

```

```{r save analysis log as csv}

try({
  write.csv(df.log_Module_2, file = paste0(output.path, "Tables/", "analysisLog.csv"), 
            row.names = F)  
}, silent = T)

```

