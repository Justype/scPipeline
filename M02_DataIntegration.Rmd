---
title: "Data Integration"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  flexdashboard::flex_dashboard:
    orientation: rows
    vertical_layout: fill
    self_contained: true
    source_code: embed
    theme: flatly
    navbar:
      - { title: "scPipeline", href: "https://github.com/NMikolajewicz/scPipeline" }
      - { title: "scMiko", href: "https://github.com/NMikolajewicz/scMiko" }  
editor_options: 
  chunk_output_type: inline
knit: (function(inputFile, encoding) {
    rmarkdown::render(input = inputFile,
      encoding = encoding,
      output_file = if (exists("user")){paste0(
        xfun::sans_ext(inputFile), '_',user, "_", 
        paste0(format(Sys.time(), "%d_%b_"), gsub(" ", "_", gsub(":", "_", format(Sys.time(), "%X"))), format(Sys.time(), "_%Y")), '.html'
      )} else {paste0(xfun::sans_ext(inputFile), '_',"Guest", "_", 
      paste0(format(Sys.time(), "%d_%b_"), gsub(" ", "_", gsub(":", "_", format(Sys.time(), "%X"))), format(Sys.time(), "_%Y")), '.html')},
      output_dir = if (exists("data.path")) paste0(data.path, "/HTML_Reports") else NULL
    )
  })
---



```{r setup, include=FALSE}

# clear global enviroment
rm(list = setdiff(ls(), c("data.path", "user")))
invisible({gc()})

# initiate timer
start.time <- proc.time()

# List of packages to load
packages2load <- c("Seurat", "sctransform",
                   "dplyr", "tidyr", "RColorBrewer", "ggplot2", "gridExtra", 
                   "DT", "flexdashboard", "future", "biomaRt", "foreach", "parallel", "doParallel", "scMiko", "reshape2", "glmGamPoi")
# load packages
invisible({lapply(packages2load, library, character.only = TRUE)})

```

```{r}
# TODO
# 241021: accomodate RDATA and RDS data 
```

```{r parameter specification}

parameter.list <- list(
  save.filename = "p14_Meso_FZD7_2000g_integrated_rpca_241021", #sctrerun_
  # save.filename = "v2_p15_HAP1_GIN_all_integrated_1500gene_rpca_artifactCorrect_devFeatures_040921.Rdata",
  integration.feature.select.method = "hvg", #option: hvg (recommended), deviance
  integration.k.filter =  200, # lower integration.k.filter value (e.g., 50) if there is few cells. Default = 200
  integration.k.anchor = 10, # this parameter influences the strength of alignment (default is 5, 20 suggested for stronger alignment). Default = 5.
  integration.k.weight = 100, # number of neighbors to consider when weighting anchor. Default = 100.
  integration.n.genes = 2000,  # number of genes for integration: 1000-3000 recommended
  integration.method = "rPCA", # options: CCA, rPCA
  integration.limit.memory = T, # specify whether to limit max memory (set T if local)
  integration.max.memory = 150, # numeric, in terms of Gb (set to 100 for cluster)
  integration.cluster.resolution = 0.5, 
  integration.rerun.SCT = F,
  pca.var.threshold = 0.9, 
  correct.artifact = T,
  # pca.method = "glmpca", #options: pca, glmpca
  n.workers = list(
    import = 4,
    integration = 1 #(limit to 1 for cluster)
  ),
  print.inline = FALSE,
  save.integrated.object = T,
  save.pdf = F,
  update.log = T
)

subset.all <- 1

# input.data <- list(
#   p15 = list(
#     file = "R579_M01_NM2_p15_HAP1_GIN_230821.Rdata",
#     # file = "R593_M01_NM2_p15_HAP1_GIN_WTonly_artifactOmitted_270821.Rdata",
#     integrate.by = "Barcode",
#     subsample = subset.all,
#     meta.feature.name = "Barcode",
#     meta.feature.recode.name = NA,
#     meta.feature.recode = NA,
#     meta.feature.filter = NA)
# )


input.data <- list(
  # R1 = list(
  #   file = "R415_M01_NM2_pilot12_meso_v2_160421.Rdata",
  #   integrate.by = "Barcode",
  #   subsample = subset.all,
  #   meta.feature.name = NA,
  #   meta.feature.recode = NA,
  #   meta.feature.filter = NA)
  FZD = list(
    file = "R514_M01_NM2_p14_Meso_Hs_030821.Rdata",
    integrate.by = "Barcode",
    subsample = subset.all,
    meta.feature.name = "Barcode",
    meta.feature.recode = NA,
    meta.feature.filter = c("FZD7", "NT")
  ))



```

```{r example of input.data formats (do not edit)}

# input.data <- list(
#   DIV7 = list(
#     file = "R579_M01_NM2_p15_HAP1_GIN_230821.Rdata",
#     integrate.by = "Replicate",
#     subsample = subset.all,
#     meta.feature.name = "Barcode",
#     meta.feature.recode.name = "Replicate",
#     meta.feature.recode = list(R1 = "R4", R2 = "R5"),
#     meta.feature.filter = NA)
#   )

```


```{r parameter assignment and assertion statements}


if ("integration.k.filter" %in% names(parameter.list)){
  stopifnot(is.numeric(parameter.list$integration.k.filter))
} else {parameter.list$integration.k.filter <- 200}

# ensure output file is specified 
if (parameter.list$save.integrated.object){
  stopifnot("save.filename" %in% names(parameter.list))
  if (!(grepl(".Rdata", parameter.list$save.filename))){
    parameter.list$save.filename <- paste0(parameter.list$save.filename, ".Rdata")
  }
}

n.cor.available <- parallel::detectCores()
for (i in 1:length(parameter.list$n.workers)){
  if (parameter.list$n.workers[[i]] > n.cor.available) parameter.list$n.workers[[i]] <- n.cor.available
}

input.labels <- names(input.data)
n.datasets <- length(input.data)

input.species <- c()
subsample_factors <- c()
input.files <- c()

for (i in 1:length(input.data)){
  
  input.list <- input.data[[i]]
  # , "species", "cluster.resolution" , species and cluster.resolution
  if (!all(c("file") %in% names(input.list))){
    stop("file must be specied for each input dataset")
  }
  
  if (!(c("subsample") %in% names(input.list))) input.list$subsample <- 1
  if (!(c("integrate.by") %in% names(input.list))) input.list$integrate.by <- NA
  if (!(c("meta.feature.name") %in% names(input.list))) input.list$meta.feature.name <- NA
  if (!(c("meta.feature.filter") %in% names(input.list))) input.list$meta.feature.filter <- NA
  if (!(c("meta.feature.recode") %in% names(input.list))) input.list$meta.feature.recode <- NA
  if (!(c("meta.feature.recode.name") %in% names(input.list))) input.list$meta.feature.recode.name <- NA
  
  
  input.data[[i]] <- input.list
  
}

```


```{r analysis log}
miko_message("Updating analysis log...")
# generate analysis log
df.log <- initiateLog("2, Data Integration")
df.log <- addLogEntry("Number of datasets", n.datasets, df.log, "n.datasets")
df.log <- addLogEntry("Subsample Factor", (as.vector(unlist(lapply(input.data, function(x) x$subsample)))), df.log, "")
df.log <- addLogEntry("Integrated Data (.Rdata)", ((as.vector(unlist(lapply(input.data, function(x) x$file))))), df.log, "input.data")
df.log <- addLogEntry("Integrated by", ((as.vector(unlist(lapply(input.data, function(x) x$integrate.by))))), df.log, "input.data")
df.log <- addLogEntry("Data Labels", (input.labels), df.log, "input.labels")
df.log <- addLogEntry("Correct artifacts", (parameter.list$correct.artifact), df.log, "correct.artifact")
df.log <- addLogEntry("K Filter", (parameter.list$integration.k.filter), df.log, "integration.k.filter")
df.log <- addLogEntry("K Anchor", (parameter.list$integration.k.anchor), df.log, "integration.k.anchor")
df.log <- addLogEntry("K Weight", (parameter.list$integration.k.weight), df.log, "integration.k.weight")
df.log <- addLogEntry("N Integration Genes", (parameter.list$integration.n.genes), df.log, "integration.n.genes")
df.log <- addLogEntry("N workers", (paste(names(parameter.list$n.workers), "=", 
                                          (purrr::map_dbl(parameter.list$n.workers,  1)), "workers",  collapse = "; ")), df.log, "n.workers")
df.log <- addLogEntry("rerun SCT",parameter.list$integration.rerun.SCT, df.log, "integration.rerun.SCT")
df.log <- addLogEntry("Proportion of variance explained by PCA",parameter.list$pca.var.threshold, df.log, "pca.var.threshold")
# df.log <- addLogEntry("PCA method",parameter.list$pca.method, df.log, "pca.method")
df.log <- addLogEntry("cluster resolution",parameter.list$integration.cluster.resolution, df.log, "integration.cluster.resolution")
df.log <- addLogEntry("memory limit",parameter.list$integration.limit.memory, df.log, "integration.limit.memory")
df.log <- addLogEntry("max memory (Gb)",parameter.list$integration.max.memory, df.log, "integration.max.memory")



```


```{r subsetting helper function}

preIntegrationSubsetting <- function(so, subset.df){
  
  rna.assay <- so@assays[["RNA"]]
  a.meta <- rna.assay@meta.features
  
  if ("SCT" %in% names(so@assays)){
    v.meta <- so@assays[["SCT"]]@meta.features
    v.assay <- "SCT"
  } else {
    v.meta <- so@assays[[DefaultAssay(so)]]@meta.features
    v.assay <- DefaultAssay(so)
  }
  
  
  gene.rep <- checkGeneRep(gNames.list, rownames(so))
  if (is.null(rna.assay@counts@Dimnames[[1]])){
    if (gene.rep == "symbol"){
      rna.assay@data@Dimnames[[1]] <- rna.assay@counts@Dimnames[[1]] <- a.meta$SYMBOL
    } else if (gene.rep == "ensembl"){
      rna.assay@data@Dimnames[[1]] <- rna.assay@counts@Dimnames[[1]] <- a.meta$ENSEMBL
    }
  }
  
  so@assays[["RNA"]] <- rna.assay
  rm (rna.assay)
  
  # check if subset input is validd
  if (is.na(unique(subset.df$field))){
    subset.flag <- FALSE
  } else if ( unique(subset.df$field) %in% names(so@meta.data)) {
    subset.flag <- TRUE
  } else {
    subset.flag <- FALSE
  }
  
  # subset data
  if (subset.flag){
    
    cur.field <- as.vector(unique(subset.df$field))
    
    match.ind <- rep(F, length(as.character(so@meta.data[[cur.field]])))
    for (i in 1:length(subset.df$subgroups)){
      pattern <- as.vector(subset.df$subgroups)[i]
      pattern <- gsub(" ", "", pattern)
      
      match.ind <- (match.ind | grepl(pattern, as.character(so@meta.data[[cur.field]]), fixed = T))
    }
    
    so <- subset(x = so, cells = colnames(so)[which(match.ind)])
    so <- UpdateSeuratObject(so)
  }
  
  so@assays[["RNA"]]@meta.features <- a.meta
  so@assays[[v.assay]]@meta.features <- v.meta
  
  
  return(so)
  
}


```


```{r load data v2}

if (!exists("data.path") & !exists("user")) {
  stop("data.path and user variables do not exist in global enviroment. Ensure that these are specified in .Rprofile. If specified, try restarting Rstudio.")
}
miko_message("Importing data...")

# TODO update to accommodate flexible inputs
dir <- "Preprocessed_Datasets/"

# get input file paths

input.files <- (as.vector(unlist(lapply(input.data, function(x) x$file))))
input.files <- paste0(data.path, dir, input.files)

# check if files exist
if (!all(file.exists(input.files))) stop("Not all input files exist\n")

# initiate lists
import.list <- list()

# start cluster
cl <- parallel::makeCluster(if (parameter.list$n.workers$import > n.datasets) n.datasets else parameter.list$n.workers$import)
doParallel::registerDoParallel(cl)

# iterate through each input file
import.list <- foreach(i = 1:length(input.files), .packages = c("scMiko", "Seurat"))  %dopar% {
  
  # get dataset name
  input.label <- input.labels[i]
  
  # ensure correct extension is present
  if (!grepl(".Rdata|.RData", input.files[i])) input.files[i] <- paste0(input.files[i], ".Rdata")
  
  # load and prep data
  load(input.files[i])
  so <- prepSeurat(so)
  
  try({
    so <- UpdateSCTAssays(so) # required after Seurat 3.1.2 update
  }, silent = T)
  
  # assign batch name
  so@meta.data[["batch"]] <- input.label
  
  # recode barcodes ##############################################################
  bc.list <- input.data[[input.label]]$meta.feature.recode
  if (all((!is.na(bc.list)) & length(bc.list) > 0)){
    
    if ((!is.na(input.data[[input.label]]$meta.feature.recode.name)) & 
        (!is.na(input.data[[input.label]]$meta.feature.name))){
      
      df.meta <- so@meta.data
      if (length(bc.list) == 0){
        df.meta[ , input.data[[input.label]]$meta.feature.recode.name] <- df.meta[ , input.data[[input.label]]$meta.feature.name]
      } else {
        df.meta[ , input.data[[input.label]]$meta.feature.recode.name] <- NA
        for (i in 1:length(bc.list)){
          df.meta[grepl(bc.list[[i]], df.meta[ , input.data[[input.label]]$meta.feature.name]) , 
                  input.data[[input.label]]$meta.feature.recode.name] <- names(bc.list)[i]
        }
        df.meta[is.na(df.meta[ ,input.data[[input.label]]$meta.feature.recode.name]) , 
                input.data[[input.label]]$meta.feature.recode.name] <- "Other"
      }
      so@meta.data <- df.meta
      input.data[[input.label]]$meta.feature.name <- input.data[[input.label]]$meta.feature.recode.name
      
    }
    
  }
  
  # subset data (if subgroup specified)
  if ((!is.na(input.data[[input.label]]$meta.feature.name)) & (!is.na(input.data[[input.label]]$meta.feature.filter))){
    subsetting.df <- data.frame(field = input.data[[input.label]]$meta.feature.name,
                                subgroups = input.data[[input.label]]$meta.feature.filter)
    so <- preIntegrationSubsetting(so, subsetting.df)
    
  } else {
    
    # ensure the dimensions are correctly defined in RNA assay (sometimes they are not?)
    a.meta <- so@assays[["RNA"]]@meta.features
    
    gene.rep <- checkGeneRep(gNames.list, rownames(so))
    if (is.null(so@assays[["RNA"]]@counts@Dimnames[[1]])){
      if (gene.rep == "symbol"){
        so@assays[["RNA"]]@counts@Dimnames[[1]] <- so@assays[["RNA"]]@data@Dimnames[[1]] <- a.meta$SYMBOL
      } else if (gene.rep == "ensembl"){
        so@assays[["RNA"]]@counts@Dimnames[[1]] <- so@assays[["RNA"]]@data@Dimnames[[1]]  <- a.meta$ENSEMBL
      }
    }
    
  }
  
  # subsample 
  if (input.data[[input.label]]$subsample < 1)  so <- downsampleSeurat(so, input.data[[input.label]]$subsample)
  
  return(list(so = so,
              gList = gNames.list,
              gNames = names(gNames.list),
              species = detectSpecies(so),
              input.label = input.label
  ))
}

# stop workers
parallel::stopCluster(cl)

# unpack results
so.list <- list()
gList <- list()
gNames <- list()

for (i in 1:length(import.list)){
  input.label <- import.list[[i]]$input.label
  so.list[[input.label]] <- import.list[[i]]$so
  gList[[input.label]] <- import.list[[i]]$gList
  gNames[[input.label]] <- import.list[[i]]$gNames
  input.data[[input.label]][["species"]] <- import.list[[i]]$species
}

# remove excess baggage
rm(import.list)


```


```{r data split, warning = FALSE}

# split seurat object by 'integrate.by' variable
input.data2 <- input.data
for (i in 1:length(input.data2)){
  
  set.name <- names(input.data2)[i]
  
  if (is.na(input.data2[[set.name]]$integrate.by)) next
  
  
  integrate.by <- input.data2[[set.name]]$integrate.by
  miko_message(paste0("Splitting '", set.name, "' by '", integrate.by, "'..."))
  input.data.current <- input.data2[[set.name]]
  gList.current <- gList[[set.name]]
  gNames.current <-  gNames[[set.name]]
  
  if (!(integrate.by %in% colnames(so.list[[set.name]]@meta.data))){
    miko_message(paste0("'", integrate.by , "' not found in '", set.name, "' dataset..."))
    next
  }
  
  # split object
  so.split <- Seurat::SplitObject(object = so.list[[set.name]], split.by = integrate.by)
  names(so.split) <- paste0(set.name, "_", names(so.split))
  
  
  new.input.data.list <- list()
  new.gList <- list()
  new.gNames <- list()
  for (j in 1:length(so.split)){
    new.input.data.list[[names(so.split)[j]]] <- input.data.current
    new.gList[[names(so.split)[j]]] <- gList.current
    new.gNames[[names(so.split)[j]]] <- gNames.current
    so.split[[j]]@meta.data[["batch"]] <- names(so.split)[j]
  }
  
  
  # update so.list
  so.list[[set.name]] <- input.data[[set.name]] <- gList[[set.name]] <- gNames[[set.name]] <- NULL
  so.list <- c(so.list, so.split)
  input.data <- c(input.data, new.input.data.list)
  gList <- c(gList, new.gList)
  gNames <- c(gNames, new.gNames)
  rm(so.split)
  invisible({gc()})
  
}

n.datasets <- length(so.list)
```


```{r rerun SCT}

sct.success <- F
try({
  if (parameter.list$integration.rerun.SCT){
    
    miko_message("Running SCTransform normalization...")
    
    pmt.present <- all(unlist(lapply(X = so.list, FUN = function(x) "percent.mt" %in% colnames(x@meta.data))))
    if (pmt.present){
      var2reg <- "percent.mt"
    } else {
      var2reg <- NULL
    }
    
    so.list <- lapply(X = so.list, FUN = SCTransform, method = "glmGamPoi", verbose = F, 
                      vars.to.regress = var2reg, variable.features.n = parameter.list$integration.n.genes)
    
  }
  sct.success <- T
  
}, silent = T)

if (sct.success & parameter.list$integration.rerun.SCT){
  miko_message("SCTransform normalization complete!")
} else if ((!sct.success) & parameter.list$integration.rerun.SCT){
  miko_message("SCTransform normalization failed.")
}




```



```{r subset common set of genes, fig.width = 12, fig.height=5}

miko_message("Subsetting common genes...")

gene.av.list <- list()

gList.consolidate <- NULL

gene.tally.list <- list()

for (i in 1:length(so.list)){
  
  set.name <- names(so.list)[i]
  which.species <- input.data[[set.name]]$species
  
  if (which.species == "Mm") {
    ens.prefix <- "ENSMUSG"
  } else if (which.species == "Hs") {
    ens.prefix <- "ENSG"
  } else {
    stop("Species must be specified as 'Mm' or 'Hs'")
  }
  
  gene.rep <- rownames(so.list[[set.name]])
  
  which.match <- grepl(ens.prefix, gene.rep)
  
  if ((!(sum(which.match) == length(gene.rep))) & (checkGeneRep(gList[[set.name]], gene.rep) != "symbol")) {
    so.list[[set.name]] <- subset(so.list[[set.name]], features = gene.rep[which.match])
  }
  
  
  gene.tally.list[[set.name]] <-   gene.rep[which.match]
  
  so.list[[set.name]] <- ens2sym.so(object = so.list[[set.name]], gNames.list = gList[[set.name]])
  
  gList.consolidate <- bind_rows(gList.consolidate, data.frame(ens = gNames[[set.name]], sym = gList[[set.name]]))
  
  gene.av.list[[set.name]] <-  rownames(so.list[[set.name]])
  
}

# gene availability tally
av.tally <- data.frame(table(unlist(gene.av.list)))
colnames(av.tally) <- c("Gene", "N")

# visualize availability
plt.gene.av.tally <- av.tally %>%
  dplyr::filter(N <= length(so.list)) %>%
  ggplot(aes(x = N)) + 
  geom_histogram(bins =  length(so.list), color = "black", fill = "grey") + 
  geom_text(stat = 'count',aes(label =..count.., vjust = -0.2)) + 
  theme_miko() + 
  xlab( paste0("Gene Representation (","/",as.integer(length(so.list)), " datasets)" )) + 
  ylab("Gene Count") + 
  labs(title = "Gene representation across datasets", subtitle = "Histogram")

# get intersecting set of genes
gene.intersect <- Reduce(intersect, gene.av.list)

# get common genes
df.common.genes <- NULL
for (i in 1:length(gene.av.list)){
  set.name <- names(gene.av.list)[i]
  current.list <- gene.av.list[[set.name]]
  
  
  for (j in 1:length(so.list)){
    current.tally <- as.character(av.tally$Gene[av.tally$N == j])
    df.common.genes <- bind_rows(df.common.genes, 
                                 data.frame(set = set.name,
                                            rep = j,
                                            n = sum(current.list %in%current.tally )
                                            
                                 ))
  }
}

df.common.genes$rep <- factor(df.common.genes$rep, levels = seq(1, length(gene.av.list)))
plt.common.bar <- df.common.genes %>%
  ggplot(aes(x = set, y = n, fill = rep)) + 
  geom_bar(stat = "identity") + 
  theme_miko(legend = T, x.axis.rotation = 45) + 
  xlab("Dataset") + ylab( paste0("Gene Count" )) + 
  labs(fill = paste0("Gene Rep.\n(","/",as.integer(length(so.list)), " datasets)" ),
       subtitle = "Stacked counts") + 
  scale_fill_manual(values = categoricalColPal(n = length(so.list)))
# ggthemes::scale_fill_gdocs()

# convert list to df
gene.av.df <- namedList2wideDF(gene.av.list)

# merge plots
plt.common.gene.plot <- cowplot::plot_grid(plt.gene.av.tally, plt.common.bar, ncol = 2, align = "hv", rel_widths = c(1.5, 2))
if (parameter.list$print.inline){
  print(plt.common.gene.plot)
}


for (i in 1:length(so.list)){
  set.name <- names(so.list)[i]
  so.list[[set.name]] <- subset(so.list[[set.name]], features = gene.intersect)
}

# generate master ens-sym mapping list
rownames(gList.consolidate) <- NULL
gList.cu <- unique(gList.consolidate)
gList.cu <- gList.cu[!duplicated(gList.cu$sym), ]
gList.cu <- gList.cu[!duplicated(gList.cu$ens), ]
gNames.list <- gList.cu$sym
names(gNames.list) <- gList.cu$ens





```


```{r get original (input) umaps, fig.width=5, fig.height=7}

# get original umap embedding and df meta

df.umap.orig <- NULL
df.meta.orig <- NULL
for (i in 1:length(so.list)){
  
  try({
    batch.name <- names(so.list)[i]
    df.umap.orig <- bind_rows(df.umap.orig, data.frame(
      batch = batch.name,
      umap.x = so.list[[batch.name]]@reductions[["umap"]]@cell.embeddings[ ,1],
      umap.y = so.list[[batch.name]]@reductions[["umap"]]@cell.embeddings[ ,2]
    ))
  }, silent = T)
  
}

set.labels <- unique(df.umap.orig$batch)
plt.original_overlay <- list()
for (i in 1:length(set.labels)){
  
  df.umap.orig.cur <- df.umap.orig
  df.umap.orig.cur$is.in <- df.umap.orig.cur$batch %in% set.labels[i]
  plt.original_overlay[[set.labels[i]]] <- df.umap.orig.cur %>%
    dplyr::arrange(is.in) %>%
    ggplot(aes(x = umap.x, y = umap.y, color = is.in)) + 
    geom_point(size = autoPointSize(nrow(df.umap.orig.cur))) + 
    scale_color_manual(values = c("TRUE" = "red", "FALSE" = "grey")) +   
    ggtitle(label = set.labels[i]) + 
    xlab("UMAP 1") +
    ylab("UMAP 2") + 
    theme_miko()
}


plt.umap.original <- df.umap.orig %>%
  ggplot(aes(x= umap.x, y = umap.y, color = batch)) + 
  geom_point(size = autoPointSize(nrow(df.umap.orig))) + 
  theme_miko(legend = T) + 
  labs(title = "Pre-Integration Data", subtitle = "UMAP") + 
  scale_fill_manual(values = categoricalColPal(n = length(so.list))) + 
  xlab("UMAP 1") + ylab("UMAP 2") + 
  theme_miko(legend = T)  + 
  theme(legend.position = "bottom")  +
  guides(color = guide_legend(override.aes = list(size = 5))) + 
  theme(legend.text=element_text(size=rel(0.5))) 
# guide_legend(size = 2)

if (parameter.list$print.inline){
  plt.umap.original
}
```



```{r relative distribution of input data}

cell_per_set <- purrr::map_dbl(so.list, ncol)

cps.df <- data.frame(batch = names(cell_per_set), n = cell_per_set)

cps.df <- cps.df %>% 
  dplyr::arrange(desc(batch)) %>%
  dplyr::mutate(prop = n / sum(cps.df$n) *100) %>%
  dplyr::mutate(ypos = cumsum(prop)- 0.5*prop )


# Basic piechart
plt.pie <- ggplot(cps.df, aes(x="", y=prop, fill=batch)) +
  geom_bar(stat="identity", width=1, color="white") +
  coord_polar("y", start=0) +
  theme_void() + 
  # geom_text(aes(y = ypos, label = n), color = "white", size=2) +
  scale_fill_manual(values = categoricalColPal(n = length(so.list))) + 
  labs(title = "Cell Frequency Per Dataset", subtitle = "Piechart")


# if (length(so.list) > 10){
plt.pie <- plt.pie   +
  guides(fill = guide_legend(override.aes = list(size = 5))) + 
  theme(legend.text=element_text(size=rel(0.5))) 
# theme(legend.position = "none")
# }

if (parameter.list$print.inline){
  plt.pie
}

```


```{r ensure cell names are unique}

# Helper function
appendCellPrefix <- function(so, prefix){
  
  if (class(so) != "Seurat") stop("Data must be Seurat Object")
  
  which.assay <- DefaultAssay(so)
  
  # store original names
  so@meta.data[["orig_cellname"]] <- colnames(so)
  
  # update cell names
  rownames(so@meta.data) <-  paste0(cell.prefix, rownames(so@meta.data))
  names(so@active.ident) <- paste0(cell.prefix,names(so@active.ident))
  so@assays[[which.assay]]@misc[["vst.out"]][["cells_step1"]] <- paste0(cell.prefix, so@assays[[which.assay]]@misc[["vst.out"]][["cells_step1"]])
  
  rownames(so@assays[[which.assay]]@misc[["vst.out"]][["cell_attr"]]) <- paste0(cell.prefix, rownames(so@assays[[which.assay]]@misc[["vst.out"]][["cell_attr"]]))
  
  
  if (!is.null(so@assays[[which.assay]]@misc[["vst.out"]][["umi_corrected"]])){
    so@assays[[which.assay]]@misc[["vst.out"]][["umi_corrected"]]@Dimnames[[2]] <- paste0(cell.prefix,  so@assays[[which.assay]]@misc[["vst.out"]][["umi_corrected"]]@Dimnames[[2]])
  }
  
  colnames(so@assays[[which.assay]]@scale.data) <- paste0(cell.prefix,colnames(so@assays[[which.assay]]@scale.data))
  
  
  for (k in 1:length(names(so@assays))){
    assay.name <- names(so@assays)[k]
    so@assays[[assay.name]]@counts@Dimnames[[2]] <- paste0(cell.prefix, so@assays[[assay.name]]@counts@Dimnames[[2]])
    so@assays[[assay.name]]@data@Dimnames[[2]] <- paste0(cell.prefix, so@assays[[assay.name]]@data@Dimnames[[2]])
  }
  
  for (k in 1:length(names(so@reductions))){
    red.name <- names(so@reductions)[k]
    rownames(so@reductions[[red.name]]@cell.embeddings) <- paste0(cell.prefix,rownames(so@reductions[[red.name]]@cell.embeddings))
  }
  
  return(so)
  
}


all.cell.names <- c()
for (i in 1:length(so.list)){
  
  # define prefix
  cell.prefix <- paste0("S", i, "_")
  
  # append to existing dataset (to ensure unique cell names prior to integration)
  try({so.list[[i]] <- appendCellPrefix(so.list[[i]], cell.prefix)}, silent = T)
  
  all.cell.names <- c(all.cell.names, colnames(so.list[[i]]))
  
}

# ensure cell names are unique
stopifnot(length(all.cell.names) == length(unique(all.cell.names)))

```



```{r feature selection, include = FALSE}

deviance.list <- list()
for (i in 1:length(so.list)){
  
  if (!("SCT" %in% names(so.list[[i]]@assays))) stop("All objects must have 'SCT' assay. Other normalization pipelines are not supported.")
  
  if (parameter.list$integration.feature.select.method == "deviance"){
    
    so <- so.list[[i]]
    DefaultAssay(so) <- "SCT"
    so.sub <- downsampleSeurat(object = so, subsample.n = 10000, verbose = F)
    
    m <- GetAssayData(so.sub, slot = "data", assay = "SCT")
    m <- m[rownames(m) %in% rownames(so.sub), ]
    m <- as.matrix(m)
    devs <- scry::devianceFeatureSelection(object = m, fam = "binomial")
    
    df.dev <- data.frame(
      gene = rownames(m),
      d = devs
    ) %>% dplyr::arrange(-d)
    df.dev <- df.dev %>% dplyr::filter(!grepl("MT", toupper(gene)))
    
    deviance.list[[names(so.list)[i]]] <- df.dev
    
    so.list[[i]]@assays[["SCT"]]@var.features <- df.dev$gene[1:round(1.5 * parameter.list$integration.n.genes)]   
    
  } else if (parameter.list$integration.feature.select.method == "hvg"){

  if (class(so.list[[i]]@assays[["SCT"]]) == "SCTAssay"){
    df.var <-  so.list[[i]]@assays[["SCT"]]@SCTModel.list[["model1"]]@feature.attributes
    if (!("SYMBOL" %in% colnames(df.var))){
      
      if (!("ENSEMBL" %in% colnames(df.var))){
        df.var$key <- stringr::str_extract( rownames(df.var),"[0-9A-Za-z-]*")
      }
      # df.mapping <-  ensembl2sym(rownames(df.var), detectSpecies(so.list[[i]]))
      e2s.success <- F
      try({
        df.mapping <-  ensembl2sym(df.var$key, detectSpecies(so.list[[i]]))
        merge.by <- "ENSEMBL"
        df.var$ENSEMBL <- df.var$key
        e2s.success <- T
      }, silent = T)
      s2e.success <- F
      try({
        df.mapping <-  sym2ens(df.var$key, detectSpecies(so.list[[i]]))
        merge.by <- "SYMBOL"
        df.var$SYMBOL <- df.var$key
        s2e.success <- T
      }, silent = T)
      
      
      df.var <- merge(df.var, df.mapping, by = merge.by)
      
    }
    df.var <- df.var[!is.na(df.var$SYMBOL), ]
    so.list[[i]]@assays[["SCT"]]@SCTModel.list[["model1"]]@feature.attributes <- df.var
    df.var <- df.var %>% dplyr::arrange(-residual_variance)
    so.list[[i]]@assays[["SCT"]]@var.features <- df.var$SYMBOL[1:round(1.5 * parameter.list$integration.n.genes)]   
  } else {
    df.var <-  so.list[[i]]@assays[["SCT"]]@meta.features 
    df.var <- df.var %>% dplyr::arrange(-sct.residual_variance)
    so.list[[i]]@assays[["SCT"]]@var.features <- df.var$SYMBOL[1:round(1.5 * parameter.list$integration.n.genes)]    
  }
    
        
  }
  
  
}


SelectDevianceFeatures <- function(deviance.list){

    
    dev.df <-  deviance.list %>%
      Reduce(function(dtf1,dtf2) left_join(dtf1,dtf2,by="gene"), .)
    dev.df <- col2rowname(dev.df, "gene")
    
    dev.df.ranked <- as.data.frame(apply(dev.df, 2, rank))
    
    xcol <- colnames(dev.df) 
    for (i in 1:ncol(dev.df)){
      dev.df2 <- dev.df %>% dplyr::arrange(-get(xcol[i]))
      dev.df[ ,xcol[i]] <- 0
      dev.df[rownames(dev.df) %in% rownames(dev.df2)[1:parameter.list$integration.n.genes],xcol[i]] <- 1
      
      
      
    }
    
    dev.df$rank <- apply(dev.df, 1, sum)
    
    dev.df.top <- dev.df %>% dplyr::top_n(parameter.list$integration.n.genes, rank)
    
    u.ranks <- unique(dev.df.top$rank)
    u.ranks <- u.ranks[order(-u.ranks)]
    
    dev.df.top.tie <- NULL
    
    for (i in 1:length(u.ranks)){
      
      dev.df.top2 <-  dev.df.top %>% dplyr::filter(rank == u.ranks[i])
      
      
      if (sum(c(nrow(dev.df.top.tie), nrow(dev.df.top2))) < parameter.list$integration.n.genes){
        dev.df.top.tie <- bind_rows(dev.df.top.tie,dev.df.top2) 
      } else {
        dev.df.ranked2 <- dev.df.ranked %>% dplyr::filter(rownames(dev.df.ranked) %in% rownames(dev.df.top2))
        dev.df.ranked2$med.rank <- apply(dev.df.ranked2, 1, median)
        nreq <- parameter.list$integration.n.genes - nrow(dev.df.top.tie)
        dev.df.ranked2.top <- dev.df.ranked2 %>% dplyr::top_n(nreq, med.rank)
        dev.df.top2 <- dev.df.top2 %>% dplyr::filter(rownames(dev.df.top2) %in% rownames(dev.df.ranked2.top))
        dev.df.top.tie <- bind_rows(dev.df.top.tie,dev.df.top2) 
        break()
      }
      
    }
    
    return(unique(rownames(dev.df.top.tie)))
    # dev.is.df <- apply(dev.df, 2, )
    
  }
  






try({
  rm(so)
  rm(so.sub)
  rm(m)
  invisible({gc()})
}, silent = T)

```


```{r integrate data, include = FALSE}

if (length(so.list) > 1){ # intergrate if there are multiple datasets provided
  
  # Select features for downstream integration 
  assay.vector <- c()
  for (i in 1:n.datasets) {
    assay.vector[i] <- DefaultAssay(so.list[[i]])
    Idents(so.list[[i]]) <- so.list[[i]]@meta.data[["seurat_clusters"]]
    so.list[[i]]@meta.data[["orig.ident"]] <-Idents(so.list[[i]])
  }
  
  # enable parallelization
  if ((parameter.list$n.workers$integration) > (length(so.list) - 1)) parameter.list$n.workers$integration <- length(so.list) - 1
  if (parameter.list$n.workers$integration > 1){
    plan(strategy = "multisession", workers = parameter.list$n.workers$integration)
  }
  if (parameter.list$integration.limit.memory) options(future.globals.maxSize = (parameter.list$integration.max.memory*20480/20 * 1024^2))    
  
  miko_message("Selecting integration features...")
  
  
  if (parameter.list$integration.feature.select.method == "deviance"){
    so.features <- SelectDevianceFeatures(deviance.list = deviance.list)
  } else {
    so.features <- SelectIntegrationFeatures(object.list = so.list,
                                             assay = assay.vector,
                                             nfeatures = parameter.list$integration.n.genes,
                                             fvf.nfeatures = parameter.list$integration.n.genes)
  }
  
  # so.features <- unique(c(so.features, "CD34"))
  
  # so.features <- unique(c(so.features, 'DGRL4', 'AFAP1L1', 'BGN', 'CALCRL', 'CD34', 'CDH5', 'CXorf36', 'DRAM1', 'ECM1', 'EGFL7', 'ENG', 'ERG', 'F2RL2', 'FAM49A', 'FAT4', 'FILIP1', 'FLI1', 'ITGA8', 'KLHL6', 'LAPTM5', 'MECOM', 'MEF2C', 'MMRN2', 'PDE4B', 'PECAM1', 'PLVAP', 'PROCR', 'PTPRB', 'PTPRE', 'RASGRP3', 'RCSD1', 'RHOJ', 'SH2D3C', 'SHE', 'SNCAIP', 'TEK', 'THSD1', 'TIMP3', 'TXNIP', 'VASH1', 'ZEB1', 'ZEB2', 'ZFPM2'))

  so.features <-  so.features[so.features %in% gene.intersect]
  # prepare seurat object for integration
  # miko_message("Preparing object for integration...")
  
  # integration.k.filter <- 200
  so.n <- unlist(lapply(so.list, ncol))
  which.too.small <- names(so.n)[so.n <= parameter.list$integration.k.filter]
  if (length(which.too.small) > 0){
    miko_message(paste0(which.too.small, collapse = ", "), " were removed due to insufficient number of cells (n < ", parameter.list$integration.k.filter, ")")
    n.datasets <- sum(so.n > parameter.list$integration.k.filter)
  }
  
  try({
    if (parameter.list$correct.artifact){
      miko_message("Identifying artifact features...")
      ag.res <-  findArtifactGenes(object = so.list, assay = NULL, features = so.features, 
                                   meta.feature = "Barcode", umi.count.threshold = 5, difference.threshold = 30, verbose = F) 
      
      so.features <- so.features[!c(so.features %in% ag.res$artifact.gene)]
      miko_message(paste0(length(ag.res$artifact.gene), " artifact genes identified and omitted: ", paste(ag.res$artifact.gene, collapse = ", ")))
    }
  }, silent = T)
  
  
  so.list2 <- tryCatch({
    
    miko_message("Preparation attempt 1....")

    # ensure SCT models use gene SYMBOL representations ########################
    so.list <- lapply(so.list, function(a){
      available.models <- names(a@assays[["SCT"]]@SCTModel.list)
      for (i in 1:length(available.models)){
        df.feature <- a@assays[["SCT"]]@SCTModel.list[[i]]@feature.attributes
        n.entries <- nrow(df.feature)
        n.ens <- sum(grepl("ENS", rownames(df.feature)))
        if (n.entries == n.ens){
          if("SYMBOL" %in% colnames(df.feature)){
            df.feature$SYMBOL[is.na(df.feature$SYMBOL)] <- "no_name"
            rownames(df.feature) <- make.unique(df.feature$SYMBOL) 
          }
        }
        a@assays[["SCT"]]@SCTModel.list[[i]]@feature.attributes <- df.feature
      }
      return(a)    
    })
    
    PrepSCTIntegration(object.list = so.list[so.n > parameter.list$integration.k.filter ], anchor.features = so.features, verbose = T)
    
  }, error = function(e){
    pmt.present <- all(unlist(lapply(X = so.list, FUN = function(x) "percent.mt" %in% colnames(x@meta.data))))
    if (pmt.present){
      var2reg <- "percent.mt"
    } else {
      var2reg <- NULL
    }
    miko_message("Preparation attempt 2....")
    so.list <- pbapply::pblapply(X = so.list[so.n > parameter.list$integration.k.filter ], FUN = SCTransform, method = "glmGamPoi", verbose = F, 
                      vars.to.regress = var2reg, variable.features.n = parameter.list$integration.n.genes)
  
    if (parameter.list$integration.feature.select.method == "deviance"){
      so.features <- SelectDevianceFeatures(deviance.list = deviance.list)
    } else {
      so.features <- SelectIntegrationFeatures(object.list = so.list,
                                               assay = assay.vector,
                                               nfeatures = parameter.list$integration.n.genes,
                                               fvf.nfeatures = parameter.list$integration.n.genes)
    }
    
    try({
      if (parameter.list$correct.artifact){
        ag.res <-  findArtifactGenes(object = so.list, assay = NULL, features = so.features, 
                                     meta.feature = "Barcode", umi.count.threshold = 5, difference.threshold = 30, verbose = F) 
        so.features <- so.features[!c(so.features %in% ag.res$artifact.gene)]
        miko_message(paste0(length(ag.res$artifact.gene), " artifact genes identified and omitted: ", paste(ag.res$artifact.gene, collapse = ", ")))
      }      
    }, silent = T)
    
    
    so.list2 <- PrepSCTIntegration(object.list = so.list, anchor.features = so.features, verbose = T)
    return(list(
      so.list = so.list2,
      so.features = so.features,
      unpack.this = T
    ))
  })
  
  if ("unpack.this" %in% names(so.list2)){
    so.features <- so.list2$so.features
    so.list2 <- so.list2$so.list
  }
  
  #########
  #########
  
  # Added override step
  # Note: solution to retain sct scaled values post-integration proposed by chlee-tabin (https://github.com/satijalab/seurat/issues/2590)
  retain.sct.scale <- F
  if (retain.sct.scale){
    for (i in 1:n.datasets) {
      so.list2[[i]][["SCT"]] <- SetAssayData( 
        object = so.list2[[i]][["SCT"]], 
        slot = "scale.data", 
        new.data = GetAssayData( so.list[[i]], assay = "SCT", slot = "scale.data" )
      )
    }    
  } 
  rm(so.list); invisible({gc()})

  # # find integration anchors
  miko_message("Finding integration anchors...")
  suppressMessages({
    suppressWarnings({
      
      if (toupper(parameter.list$integration.method) == toupper("CCA")) {
        so.anchors <- FindIntegrationAnchors(object.list = so.list2, normalization.method = "SCT",
                                             anchor.features = so.features, verbose = FALSE, 
                                             k.filter = parameter.list$integration.k.filter,  k.anchor = parameter.list$integration.k.anchor )
      } else if (toupper(parameter.list$integration.method) == toupper("rPCA")) {
        
        # run PCA on each object in the list (required for reicprocal PCA workflow)
        so.list2 <- pbapply::pblapply(X = so.list2, FUN = function(x) {
          x <- RunPCA(x, features = so.features, verbose = FALSE)
        })          
        
        # identify anchors and integrate datasets
        so.anchors <- FindIntegrationAnchors(object.list = so.list2, normalization.method = "SCT", reduction = "rpca",
                                             k.filter = parameter.list$integration.k.filter, k.anchor = parameter.list$integration.k.anchor , 
                                             anchor.features = so.features, verbose = FALSE)
        
        
        
      }
    })
  })
  
  rm(so.list2); invisible({gc()})
  
  # integrate data
  miko_message("Integrating data...")
  so <- IntegrateData( 
    anchorset = so.anchors, 
    normalization.method = "SCT",
    k.weight = parameter.list$integration.k.weight, # added 24.08.21
    verbose = T
  )
  
  # Switch to integrated assay. Variable features are automatically set during IntegrateData()
  DefaultAssay(so) <- "integrated"
} else {
  so <- so.list[[1]]
  rm(so.list)
}

# integration.n.genes.effect <- length(which.features.integrate)
n.anchors.effect <- length(so.anchors@command@params[["anchor.features"]])

# remove baggage
rm(so.anchors);invisible({gc()})


```




```{r PCA}

# Run PCA
miko_message("Running PCA...")
so <- RunPCA(so, verbose = FALSE)

# proportion of variance explained by each PC 
pc.std <- so@reductions[["pca"]]@stdev
pc.var <- pc.std^2
pc.prop_var <- pc.var/sum(pc.var)
pc.cum_sum <- cumsum(pc.prop_var)
pc.id <- c(1:50)
scree.var <- data.frame(pc.id, pc.prop_var, pc.cum_sum)

# Number of dims to use
pca.var.threshold <- parameter.list$pca.var.threshold
pca.prop <- propVarPCA(so)
# pc.n_relevant_components <- max(pca.prop$pc.id[pca.prop$pc.cum_sum<pca.var.threshold])+1

pca.elbow.low <- 0.015
parameter.list$pca.component.select.method = "cum_var"
if (parameter.list$pca.component.select.method == "elbow"){
  pc.n_relevant_components <- pcaElbow(pca.prop$pc.prop_var, low = pca.elbow.low, max.pc = 0.9)
} else if (parameter.list$pca.component.select.method == "cum_var"){
  pc.n_relevant_components <- max(pca.prop$pc.id[pca.prop$pc.cum_sum<pca.var.threshold])+1
} else {
  pc.n_relevant_components <- pcaElbow(pca.prop$pc.prop_var, low = pca.elbow.low, max.pc = 0.9)
}

#pc.n_relevant_components <- 30

# generate Scree Plot
plt.scree1 <- ggplot(scree.var, aes(x = pc.id, y = pc.prop_var)) + 
  geom_point() +  
  theme(legend.position="right") +
  geom_vline(xintercept = pc.n_relevant_components+0.5, color = 'red') + 
  ggtitle("Scree Plot") + 
  xlab("Principal Components") + 
  ylab("Variance Explained (proportion)") + 
  theme_miko()

plt.scree2 <- ggplot(scree.var, aes(x = pc.id, y = pc.cum_sum)) + 
  theme(legend.position="right") +
  geom_vline(xintercept = pc.n_relevant_components+0.5, color = 'red') + 
  # geom_hline(yintercept =pca.var.threshold, color = 'red') + 
  geom_point() +
  ylim(0, 1) + 
  ggtitle("Cumulative Variance Explained") + 
  # ggtitle(paste(pc.n_relevant_components, "PCs (", round(100*pc.cum_sum[pc.n_relevant_components],2), "%) of variance")) + 
  xlab("Principal Components") + 
  ylab("Cumulative Variance Explained (proportion)") + theme_miko()


if (parameter.list$print.inline){
  print(plt.scree1)
  print(plt.scree2)
}

```

```{r identify artifact genes, fig.width=12, fig.height=4}

do.artifact <- F

if (do.artifact){
  object <- so
  # assay = DefaultAssay(object)
  DefaultAssay(object) <- "integrated"
  meta.feature = "Barcode"
  umi.count.threshold = 5
  assay = NULL; features = NULL
  difference.threshold = 100
  
  DefaultAssay(so) <- "integrated"
  
  ag.res <-  findArtifactGenes(object = so, assay = NULL, features = NULL, meta.feature = "Barcode", umi.count.threshold = 5, difference.threshold = 100, verbose = T)
  
  ag.res$plot
 
}
```


```{r cluster data}
# Find clusters ---------------------------------------------------------------------------
miko_message("Finding nearest neighbors...")
so <- FindNeighbors(object = so, reduction = "pca", dims = 1:pc.n_relevant_components)

# integration.cluster.resolution <- 1
miko_message("Clustering data...")
so <- FindClusters(object = so, resolution = parameter.list$integration.cluster.resolution, 
                   verbose = 0, algorithm = 1, modularity.fxn = 1, group.singletons = T)

```

```{r run_umap, fig.width=8, fig.height=8}

# Run umap and generate plot --------------------------------------------------------------------
nDim_umap <- pc.n_relevant_components
so <- RunUMAP(so, dims = 1:nDim_umap)
plt.integration <- DimPlot(so, reduction = "umap",group.by = "batch", pt.size = autoPointSize(ncol(so)))  + ggtitle(label = "UMAP") +
  xlab("UMAP 1") + ylab("UMAP 2")

plt.integration <-  plt.integration + 
  theme_miko(legend = T) + 
  labs(title = "Integrated Data", subtitle = "UMAP") + 
  scale_fill_manual(values = categoricalColPal(n = n.datasets)) + 
  theme(legend.position = "bottom") +
  guides(color = guide_legend(override.aes = list(size = 5))) + 
  theme(legend.text=element_text(size=rel(0.5))) 


if (parameter.list$print.inline){
  print(plt.integration)
}

```


```{r umap by pilot, fig.width=16, fig.height=15, include = FALSE}

plt.integration_overlay <- list()
# show umap stratified by batch


set.labels <- unique(so@meta.data[["batch"]])
for (i in 1:length(set.labels)){
  plt.integration_overlay[[set.labels[i]]] <- DimPlot(so, 
                                                      reduction = "umap", 
                                                      label = F, cells.highlight = as.vector(which(so@meta.data[["batch"]] == set.labels[i])), 
                                                      label.size = 4,
                                                      pt.size = autoPointSize(ncol(so)),
                                                      sizes.highlight = autoPointSize(ncol(so)))  +   
    ggtitle(label = set.labels[i]) + 
    xlab("UMAP 1") +
    ylab("UMAP 2") + 
    theme_miko()
}


plt.overlay.combine <- list()
for (i in 1:length(plt.original_overlay)){
  
  set.name <- names(plt.original_overlay)[i]
  
  plt.overlay.combine[[set.name]] <- cowplot::plot_grid(plt.original_overlay[[set.name]] + labs(subtitle = "Pre-integration"), 
                                                        plt.integration_overlay[[set.name]] + labs(subtitle = "Post-integration", title = ""), nrow = 1)
  
}

plt.ncol <- if (n.datasets > 3) 4 else  n.datasets
plt.integration_by_batch <- cowplot::plot_grid(plotlist = plt.integration_overlay, ncol = plt.ncol)

if (parameter.list$print.inline){
  print(plt.integration_by_batch)
}

# plt.integration_overlay

```

```{r umap by cluster, warning=FALSE}


# show umap stratified by cluster
plt.umap_by_cluster <- DimPlot(so, 
                               reduction = "umap",  
                               label.size = 4, 
                               label = TRUE, 
                               pt.size = autoPointSize(ncol(so)))  +   
  ggtitle("Clusters") + 
  xlab("UMAP 1") + 
  ylab("UMAP 2") 

plt.umap_by_cluster <- plt.umap_by_cluster + 
  theme_miko(legend = T) + 
  labs(title = "Integrated Dataset", subtitle = "UMAP stratfiied by clusters") + 
  scale_color_manual(values = categoricalColPal(n = length(unique(so@meta.data$seurat_clusters))))  

if (parameter.list$print.inline){
  print(plt.umap_by_cluster)
}


```



```{r umap by barcode, fig.height=5, fig.width=10}

plt.umap_by_barcode <- DimPlot(so, split.by = "batch", group.by = "Barcode",  pt.size = 0.02) + 
  xlab("UMAP 1") + ylab("UMAP 2")+ theme(legend.position = "bottom")

plt.umap_by_batch <- DimPlot(so, group.by = "batch",  pt.size = 0.02) + 
  xlab("UMAP 1") + ylab("UMAP 2")  +
  guides(color = guide_legend(override.aes = list(size = 5))) + 
  theme(legend.text=element_text(size=rel(0.5))) 

if (parameter.list$print.inline){
  print(plt.umap_by_barcode)
  plt.umap_by_batch
}


```


```{r function for long to wide annotation table}

long2wide <- function(df){
  
  # create wide version (for output to excel), reorder factors, and get basic descriptive stats
  df.wide <- dcast(df, cluster ~ batch, value.var = "n")
  df.wide[is.na(df.wide)] <- 0
  reordered_factors <- order(as.numeric(as.vector(df.wide$cluster)))
  df.wide <- df.wide[reordered_factors, ]
  rownames(df.wide) <- df.wide$cluster
  df.wide <- as.data.frame(t(df.wide))
  df.wide <- df.wide[2:dim(df.wide)[1], ]
  df_id_rnames <- rownames(df.wide)
  colnames(df.wide) <- paste("cluster", colnames(df.wide), sep = "")
  df.wide <- data.frame(apply(df.wide, 2, function(x) as.numeric(as.character(x))))
  df.wide[dim(df.wide)[1]+1, ] <- apply(df.wide, 2, sum)
  rownames(df.wide) <- c(df_id_rnames, "TOTAL")
  df.wide[, dim(df.wide)[2]+1] <- apply(df.wide, 1, sum)
  colnames(df.wide)[dim(df.wide)[2]] <- "TOTAL"
  
  return(df.wide)
}

```

```{r compare cluster composition}


batches <- as.vector(so@meta.data[["batch"]])
clusters <- as.vector(so@meta.data[["seurat_clusters"]])

u.batches <- unique(batches)
u.clusters <- unique(clusters)

df.cluster_comp <- data.frame(batch = batches, 
                              cluster = clusters)


df.tally <- df.cluster_comp %>%
  group_by(cluster, batch) %>%
  tally() %>%
  mutate(freq = n/sum(n))

df_for_wide <- df.tally
# df.all_id_wide <- long2wide(df_for_wide)

df.cluster_annotations <- df.tally

u.batches <- unique(batches)
n.batches <- length(u.batches)

color_count <- max(n.batches)
my_cols = colorRampPalette(brewer.pal(color_count, "Set2"))(color_count)

# ensure that clusters are ordered numerically
reordered_clusters <- order(as.numeric(as.vector(df.cluster_annotations$cluster)))
df.cluster_annotations <- df.cluster_annotations[reordered_clusters, ]
df.cluster_annotations$cluster <- as.numeric(as.vector(df.cluster_annotations$cluster))
cluster_chart_labels <- unique(df.cluster_annotations$cluster)

# df.cluster_annotations_int <- df.cluster_annotations_int[order(levels(df.cluster_annotations_int$cluster_membership_int)), ]
plt.cluster_composition <- ggplot(df.cluster_annotations, 
                                  aes(x = cluster, 
                                      fill = batch, 
                                      y = freq)) +
  geom_bar(position = "fill", stat = "identity") + 
  scale_x_continuous("Cluster", labels = as.character(cluster_chart_labels), breaks = cluster_chart_labels) + 
  xlab("Cluster ID") + ylab("Cluster Representation") + 
  ggtitle("Cluster Composition") + 
  scale_fill_manual(values = categoricalColPal(n = ulength(df.cluster_annotations$batch))) + 
  # ggthemes::scale_fill_ptol() + 
  theme_miko(legend = T) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))


try({
  plt.cluster_composition <- plt.cluster_composition +
    theme(legend.position = "bottom") + 
    guides(fill = guide_legend(override.aes = list(size = 5))) + 
    theme(legend.text=element_text(size=rel(0.5))) 
}, silent = T)
# scale_fill_manual(values = my_cols) + 


if (parameter.list$print.inline){
  plt.cluster_composition
}


```



```{r central log}

# update central log
run.id <- NULL
if (!exists("user")) user <- "guest"

clog.update.success <- F
if (parameter.list$update.log){
  try({
    run.id <-  updateCentralLog(Module = "M02", 
                                input.data = paste(unlist(lapply(input.data, function(x) x$file)), collapse = ", "), 
                                input.subset = NA, 
                                pdf.flag = parameter.list$save.pdf)
    clog.update.success <-  T
  }, silent = F) 
}

if (is.null(run.id))  {
  miko_message("Central log update was unsuccessful :(\n")
  run.id <- paste("M02_", user, "_r", paste0(format(Sys.time(), '%s')), sep = "", collapse = "")
}

```

```{r setup output directories}

# output path
if (!exists("data.path")) data.path = ""
output.path <- paste0(data.path, "Module_Outputs/", paste0(run.id,"_",format(Sys.time(), '%d%m%y'), "/"))

# create output directories
dir.create(output.path)
dir.create(paste0(output.path, "Tables/"))
if (parameter.list$save.pdf) dir.create(paste0(output.path, "PDF/"))

```

Data Integration
===================================== 

Sidebar {.sidebar}
-------------------------------------

**scRNAseq Data Integration**

**Description**: Create an integrated dataset for downstream scRNAseq analysis. 

**Method**: Cross-dataset cell pairs (i.e., anchors) are identified based on corresponding biological states and these anchors are used to harmonize datasets into a single dataset, thereby correcting for technical differences across datasets (i.e., batch effect correction). 

**Citation**: Stuart, T., Butler, A., Hoffman, P., Hafemeister, C., Papalexi, E., Mauck III, W. M., ... & Satija, R. (2019). Comprehensive integration of single-cell data. *Cell*, 177(7), 1888-1902



Row {.tabset data-height=500}
-------------------------------------

### Integration

```{r plt.integration, fig.width=12, fig.height=7}

umap.oi <- cowplot::plot_grid(plt.umap.original , plt.integration, align = "v",  ncol = 2)
print(umap.oi)

try({
  savePDF(file.name = paste0(output.path, "PDF/", "M02_UMAP_input_vs_integrated.pdf"), 
          plot.handle = umap.oi, 
          fig.width = 12, fig.height = 7, save.flag = parameter.list$save.pdf)
}, silent = T)

```

### Number of Cells per Dataset
```{r n pie chart}
plt.pie
```

```{r save pie}

try({
  savePDF(file.name = paste0(output.path, "PDF/", "M02_pie_composition.pdf"), 
          plot.handle = plt.pie, save.flag = parameter.list$save.pdf)
}, silent = T)

```


### PCA 
```{r plt.PCA, fig.width=8, fig.height=4, dpi = 72}

plt.scree.combo <- cowplot::plot_grid(plt.scree1, plt.scree2, ncol=2)
print(plt.scree.combo)

try({
  savePDF(file.name = paste0(output.path, "PDF/", "M02_scree_plot.pdf"), 
          plot.handle = plt.scree.combo, 
          fig.width = 8, fig.height = 4, save.flag = parameter.list$save.pdf)
}, silent = T)

```

### Integrated Clusters

```{r plt.cluster_composition, fig.width = 12, fig.height = 4}

plt.comp.combo <- cowplot::plot_grid(plt.umap_by_cluster + theme(legend.position = "none"), 
                                     plt.cluster_composition, rel_widths = c(1,1.5),nrow = 1,  axes = "tb")
print(plt.comp.combo)

try({
  savePDF(file.name = paste0(output.path, "PDF/", "M02_sample_composition.pdf"), 
          plot.handle = plt.comp.combo, 
          fig.width = 12, fig.height = 4, save.flag = parameter.list$save.pdf)
}, silent = T)

```

### Gene Intersection
```{r n common geneplot, fig.width = 12, fig.height =5}
print(plt.common.gene.plot)
```

```{r save common gene plot}

try({
  savePDF(file.name = paste0(output.path, "PDF/", "M02_gene_intersection.pdf"), 
          plot.handle = plt.common.gene.plot, fig.width = 12, fig.height =5, save.flag = parameter.list$save.pdf)
}, silent = T)

```



Row {.tabset data-height=500}
-------------------------------------

```{r sample-specific plots}

out <- lapply(seq_along(plt.overlay.combine), function(i) {
  
  s1 <- names(plt.overlay.combine)[i]
  s2 <- paste0("plt.overlay.combine[[", i, "]]")
  
  a1 <- knitr::knit_expand(text = sprintf("### %s\n", s1))  # tab header
  a2 <- knitr::knit_expand(text = sprintf("\n```{r %s, message=FALSE, warning=FALSE, fig.width = 14, fig.height=6}",  #fig.width = 8, fig.height=8, 
                                          paste("inOver_", i, sep = "")))
  a3 <- knitr::knit_expand(text = sprintf("\n %s", s2)) 
  a4 <- knitr::knit_expand(text = "\n```\n") # end r chunk
  
  paste(a1, a2, a3, a4, collapse = '\n') # collapse together all lines with newline separator
  
})

```

`r paste(knitr::knit(text = paste(out, collapse = '\n')))`

Row
-----------------------------------------------------------------------

### Datasets
```{r valuebox1}
valueBox(n.datasets)
```

### Cells
```{r valuebox1-2}
valueBox(ncol(so))
```

### Integrated Genes
```{r valuebox2}
valueBox(length(so.features))
```

### Total Genes
```{r valuebox3}
try({valueBox(nrow(so@assays[["RNA"]]))}, silent = T)
```






Tables
===================================== 

### Integrated Genes

Genes used for data integration.
```{r integ genes}

try({
  flex.asDT(data.frame(gene = so.features))
}, silent = T)

```

### Gene Intersection Tally
Tally of genes represented across datasets. 
```{r available genes}

try({
  flex.asDT(av.tally)
}, silent = T)

```

### Gene Intersection

Genes represented in all datasets. 
```{r gene int table}

try({
  flex.asDT(gene.av.df)
}, silent = T)

```


```{r tally table}

try({
  table.name <- "gene_intersection_tally_table.csv"
  write.csv(av.tally, file = paste0(output.path, "Tables/", table.name),
            row.names = F)
}, silent = T)

try({
  table.name <- "gene_intersection_table.csv"
  write.csv(gene.av.df, file = paste0(output.path, "Tables/", table.name),
            row.names = F)
}, silent = T)

```


```{r save results}

# Data Integration Method
if (parameter.list$integration.method == "CCA"){int_method <- "CCA"} else if (parameter.list$integration.method == "rPCA"){int_method <- "rPCA"}
df.log <- addLogEntry("Integration Method", parameter.list$integration.method, df.log, "integration.method")

end.time <- proc.time()
elapsed.time <- round((end.time - start.time)[[3]], 2)
df.log <- addLogEntry("Run Time (s)", elapsed.time, df.log, "elapsed.time")
df.log <- addLogEntry("Results Saved", parameter.list$save.integrated.object, df.log, "save.integrated.object")
df.log <- addLogEntry("PDFs Saved", parameter.list$save.pdf, df.log, "save.pdf")

df.log <- addLogEntry("Run Identifier", run.id, df.log, "run.id")
df.log <- addLogEntry("User", user, df.log, "user")
df.log <- addLogEntry("Central Log Updated", clog.update.success, df.log, "clog.update.success")

parameter.list$save.filename <- paste0(run.id, "_", parameter.list$save.filename)
df.log <- addLogEntry("Output File", (parameter.list$save.filename), df.log, "save.filename")

df.log_Module_2 <- df.log

if (parameter.list$save.integrated.object == TRUE){
  if (!exists("dir")){dir <- ""} 
  save(so, gNames.list, df.log_Module_2, file = paste0(data.path, dir, parameter.list$save.filename))
}

```


Log (Module 2)
===================================== 

```{r table.log_current}

knitr::kable(df.log_Module_2)

```

```{r save analysis log as csv}

try({
  write.csv(df.log_Module_2, file = paste0(output.path, "Tables/", "analysisLog.csv"), 
            row.names = F)  
}, silent = T)

```

